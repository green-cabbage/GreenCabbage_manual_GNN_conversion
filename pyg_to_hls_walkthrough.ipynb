{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1bc2776",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9b384d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: Conv1d\n",
      "func: <function parse_conv1d_layer at 0x7f8c336de3b0>\n",
      "layer: Conv2d\n",
      "func: <function parse_conv2d_layer at 0x7f8c336de440>\n",
      "layer: Linear\n",
      "func: <function parse_linear_layer at 0x7f8c336de560>\n",
      "layer: Softmax\n",
      "func: <function parse_activation_layer at 0x7f8c336de4d0>\n",
      "layer: ReLU\n",
      "func: <function parse_activation_layer at 0x7f8c336de4d0>\n",
      "layer: BatchNorm2d\n",
      "func: <function parse_batchnorm_layer at 0x7f8c336de320>\n",
      "layer: BatchNorm1d\n",
      "func: <function parse_batchnorm_layer at 0x7f8c336de320>\n",
      "handler args: ('NodeBlock',)\n",
      "handler args: ('EdgeBlock',)\n",
      "handler args: ('EdgeAggregate',)\n",
      "handler args: ('ResidualBlock',)\n",
      "handler args: ('NodeEncoder',)\n",
      "handler args: ('EdgeEncoder',)\n",
      "layer: NodeBlock\n",
      "func: <function parse_NodeBlock at 0x7f8c336eccb0>\n",
      "layer: EdgeBlock\n",
      "func: <function parse_EdgeBlock at 0x7f8c336ece60>\n",
      "layer: EdgeAggregate\n",
      "func: <function parse_EdgeAggregate at 0x7f8c336ef050>\n",
      "layer: ResidualBlock\n",
      "func: <function parse_ResidualBlock at 0x7f8c336ef200>\n",
      "layer: NodeEncoder\n",
      "func: <function parse_NodeEncoder at 0x7f8c336ef3b0>\n",
      "layer: EdgeEncoder\n",
      "func: <function parse_EdgeEncoder at 0x7f8c336ef560>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from hls4ml.utils.config import config_from_pyg_model\n",
    "from hls4ml.converters import convert_from_pyg_model\n",
    "# module_path = os.path.abspath(os.path.join('../pyg_to_hls_hls4ml/hls4ml/utils'))\n",
    "# print(module_path)\n",
    "# if module_path not in sys.path:\n",
    "#     sys.path.append(module_path)\n",
    "# from config import config_from_pyg_model\n",
    "\n",
    "# module_path = os.path.abspath(os.path.join('../pyg_to_hls_hls4ml/hls4ml'))\n",
    "# print(module_path)\n",
    "# if module_path not in sys.path:\n",
    "#     sys.path.append(module_path)\n",
    "# from converters import convert_from_pyg_model\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# locals\n",
    "from utils.models.interaction_network_pyg import InteractionNetwork\n",
    "from model_wrappers import model_wrapper\n",
    "from utils.data.dataset_pyg import GraphDataset\n",
    "from utils.data.fix_graph_size import fix_graph_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb118bac",
   "metadata": {},
   "source": [
    "### PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24082fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R1: RelationalModel(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=15, out_features=40, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=40, out_features=40, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=40, out_features=5, bias=True)\n",
      "  )\n",
      ")\n",
      "R1.layers: Sequential(\n",
      "  (0): Linear(in_features=15, out_features=40, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=40, out_features=40, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=40, out_features=5, bias=True)\n",
      ")\n",
      "R1.layers.0: Linear(in_features=15, out_features=40, bias=True)\n",
      "R1.layers.1: ReLU()\n",
      "R1.layers.2: Linear(in_features=40, out_features=40, bias=True)\n",
      "R1.layers.3: ReLU()\n",
      "R1.layers.4: Linear(in_features=40, out_features=5, bias=True)\n",
      "O: ObjectModel(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=5, out_features=40, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=40, out_features=40, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=40, out_features=5, bias=True)\n",
      "  )\n",
      ")\n",
      "O.layers: Sequential(\n",
      "  (0): Linear(in_features=5, out_features=40, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=40, out_features=40, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=40, out_features=5, bias=True)\n",
      ")\n",
      "O.layers.0: Linear(in_features=5, out_features=40, bias=True)\n",
      "O.layers.1: ReLU()\n",
      "O.layers.2: Linear(in_features=40, out_features=40, bias=True)\n",
      "O.layers.3: ReLU()\n",
      "O.layers.4: Linear(in_features=40, out_features=5, bias=True)\n",
      "R2: RelationalModel(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=15, out_features=40, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=40, out_features=40, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=40, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "R2.layers: Sequential(\n",
      "  (0): Linear(in_features=15, out_features=40, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=40, out_features=40, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=40, out_features=1, bias=True)\n",
      ")\n",
      "R2.layers.0: Linear(in_features=15, out_features=40, bias=True)\n",
      "R2.layers.1: ReLU()\n",
      "R2.layers.2: Linear(in_features=40, out_features=40, bias=True)\n",
      "R2.layers.3: ReLU()\n",
      "R2.layers.4: Linear(in_features=40, out_features=1, bias=True)\n",
      "res_block: ResidualBlock()\n",
      "node_encoder: Linear(in_features=3, out_features=5, bias=True)\n",
      "edge_encoder: Linear(in_features=4, out_features=5, bias=True)\n"
     ]
    }
   ],
   "source": [
    "torch_model = InteractionNetwork(aggr=\"add\", flow=\"source_to_target\", hidden_size=40)\n",
    "# torch_model_dict = torch.load(\"trained_models//IN_pyg_small_add_source_to_target_40_state_dict.pt\")\n",
    "# torch_model.load_state_dict(torch_model_dict)\n",
    "\n",
    "for name, submodule in torch_model.named_modules():\n",
    "    if name != \"\":\n",
    "        print(f\"{name}: {submodule}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2293596d",
   "metadata": {},
   "source": [
    "We can see that this specific GNN is composed of 3 submodules:\n",
    "- The first submodule, \"R1\", is a \"RelationalModel\" a.k.a. an \"EdgeBlock\"\n",
    "- The second submodule, \"O\", is an \"ObjectModel\" a.k.a. a \"NodeBlock\"\n",
    "- The third submodule, \"R2\" is another \"RelationalModel\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e49e754",
   "metadata": {},
   "source": [
    "### HLS Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e798bed7",
   "metadata": {},
   "source": [
    "hls4ml cannot infer the *order* in which these submodules are called within the pytorch model's \"forward()\" function. We have to manually define this information in the form of an ordered-dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18b64da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward_dict: defines the order in which graph-blocks are called in the model's 'forward()' method\n",
    "forward_dict = OrderedDict()\n",
    "forward_dict[\"node_encoder\"] = \"NodeEncoder\"\n",
    "forward_dict[\"edge_encoder\"] = \"EdgeEncoder\"\n",
    "forward_dict[\"R1\"] = \"EdgeBlock\"\n",
    "forward_dict[\"O\"] = \"NodeBlock\"\n",
    "forward_dict[\"res_block\"] = \"ResidualBlock\"\n",
    "forward_dict[\"R2\"] = \"EdgeBlock\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "6dca8d9f",
   "metadata": {},
   "source": [
    "hls4ml creates a hardware implementation of the GNN, which can only be represented using fixed-size arrays. This restriction also applies to the inputs and outputs of the GNN, so we must define the size of the graphs that this hardware GNN can take as input**, again in the form of a dictionary. \n",
    "\n",
    "**Graphs of a different size can be padded or truncated to the appropriate size using the \"fix_graph_size\" function. In this notebook, padding/truncation is  done in the \"Data\" cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baa5a9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_dim = 5\n",
    "graph_dims = {\n",
    "        \"n_node\": 28,\n",
    "        \"n_edge\": 37,\n",
    "#         \"node_dim\": 3,\n",
    "        \"node_attr\": 3,\n",
    "        \"node_dim\": common_dim,\n",
    "#         \"edge_dim\": 4,\n",
    "        \"edge_attr\": 4,\n",
    "    \"edge_dim\":common_dim\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "623f2192",
   "metadata": {},
   "source": [
    "Armed with our pytorch model and these two dictionaries**, we can create the HLS model. \n",
    "\n",
    "**If there is some activation function after the output of the final GNN-submodule, we also have to pass the type of this activation through the \"activate_final\" parameter of \"convert_from_pyg_model\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da3c3fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PygModelReader node_dim: 5\n",
      "PygModelReader edge_dim: 5\n",
      "PygModelReader node_attr: 3\n",
      "PygModelReader edge_attr: 4\n",
      "node encoder layer_dict['n_in']: 3\n",
      "node encoder layer_dict['n_out']: 5\n",
      "edge encoder b4 update_dict[\"last_edge_update\"]: edge_attr\n",
      "edge encoder after update_dict[\"last_edge_update\"]: layer5_out\n",
      "EdgeBlock update_dict[\"last_edge_update\"] b4: layer5_out\n",
      "EdgeBlock update_dict[\"last_edge_update\"] after: layer6_out\n",
      "type(update_dict): <class 'dict'>\n",
      "layer_dict['inputs']: ['layer4_out', 'layer8_out']\n",
      "EdgeBlock update_dict[\"last_edge_update\"] b4: layer6_out\n",
      "EdgeBlock update_dict[\"last_edge_update\"] after: layer10_out\n",
      "NodeEncoder name: node_encoder\n",
      "self.state_dict.keys(): odict_keys(['R1.layers.0.weight', 'R1.layers.0.bias', 'R1.layers.2.weight', 'R1.layers.2.bias', 'R1.layers.4.weight', 'R1.layers.4.bias', 'O.layers.0.weight', 'O.layers.0.bias', 'O.layers.2.weight', 'O.layers.2.bias', 'O.layers.4.weight', 'O.layers.4.bias', 'R2.layers.0.weight', 'R2.layers.0.bias', 'R2.layers.2.weight', 'R2.layers.2.bias', 'R2.layers.4.weight', 'R2.layers.4.bias', 'node_encoder.weight', 'node_encoder.bias', 'edge_encoder.weight', 'edge_encoder.bias'])\n",
      "layer_name + '.' + var_name: node_encoder.weight\n",
      "self.state_dict.keys(): odict_keys(['R1.layers.0.weight', 'R1.layers.0.bias', 'R1.layers.2.weight', 'R1.layers.2.bias', 'R1.layers.4.weight', 'R1.layers.4.bias', 'O.layers.0.weight', 'O.layers.0.bias', 'O.layers.2.weight', 'O.layers.2.bias', 'O.layers.4.weight', 'O.layers.4.bias', 'R2.layers.0.weight', 'R2.layers.0.bias', 'R2.layers.2.weight', 'R2.layers.2.bias', 'R2.layers.4.weight', 'R2.layers.4.bias', 'node_encoder.weight', 'node_encoder.bias', 'edge_encoder.weight', 'edge_encoder.bias'])\n",
      "layer_name + '.' + var_name: node_encoder.bias\n",
      "EdgeEncoder name: edge_encoder\n",
      "self.state_dict.keys(): odict_keys(['R1.layers.0.weight', 'R1.layers.0.bias', 'R1.layers.2.weight', 'R1.layers.2.bias', 'R1.layers.4.weight', 'R1.layers.4.bias', 'O.layers.0.weight', 'O.layers.0.bias', 'O.layers.2.weight', 'O.layers.2.bias', 'O.layers.4.weight', 'O.layers.4.bias', 'R2.layers.0.weight', 'R2.layers.0.bias', 'R2.layers.2.weight', 'R2.layers.2.bias', 'R2.layers.4.weight', 'R2.layers.4.bias', 'node_encoder.weight', 'node_encoder.bias', 'edge_encoder.weight', 'edge_encoder.bias'])\n",
      "layer_name + '.' + var_name: edge_encoder.weight\n",
      "self.state_dict.keys(): odict_keys(['R1.layers.0.weight', 'R1.layers.0.bias', 'R1.layers.2.weight', 'R1.layers.2.bias', 'R1.layers.4.weight', 'R1.layers.4.bias', 'O.layers.0.weight', 'O.layers.0.bias', 'O.layers.2.weight', 'O.layers.2.bias', 'O.layers.4.weight', 'O.layers.4.bias', 'R2.layers.0.weight', 'R2.layers.0.bias', 'R2.layers.2.weight', 'R2.layers.2.bias', 'R2.layers.4.weight', 'R2.layers.4.bias', 'node_encoder.weight', 'node_encoder.bias', 'edge_encoder.weight', 'edge_encoder.bias'])\n",
      "layer_name + '.' + var_name: edge_encoder.bias\n",
      "self.attributes.keys(): dict_keys(['name', 'n_node', 'n_edge', 'node_dim', 'edge_dim', 'activate_final', 'activation', 'n_layers', 'out_dim', 'class_name', 'inputs', 'outputs', 'accum_t'])\n",
      "self.model.output_vars.keys(): dict_keys(['node_attr', 'edge_attr', 'edge_index', 'layer4_out', 'layer5_out'])\n",
      "self.edge_dim_cppname: N_LAYER_2_4\n",
      "self.attributes.keys(): dict_keys(['name', 'n_node', 'n_edge', 'node_dim', 'edge_dim', 'activate_final', 'activation', 'n_layers', 'out_dim', 'class_name', 'inputs', 'outputs', 'accum_t'])\n",
      "self.inputs[1]: layer7_out\n",
      "self.model.output_vars.keys(): dict_keys(['node_attr', 'edge_attr', 'edge_index', 'layer4_out', 'layer5_out', 'layer6_out', 'layer7_out'])\n",
      "self.edge_dim_cppname: N_LAYER_2_4\n",
      "rank: 2\n",
      "resblock name: aggr9\n",
      "self.attributes.keys(): dict_keys(['name', 'n_node', 'n_edge', 'node_dim', 'edge_dim', 'activate_final', 'activation', 'n_layers', 'out_dim', 'class_name', 'inputs', 'outputs', 'accum_t'])\n",
      "self.model.output_vars.keys(): dict_keys(['node_attr', 'edge_attr', 'edge_index', 'layer4_out', 'layer5_out', 'layer6_out', 'layer7_out', 'layer8_out', 'layer9_out'])\n",
      "self.edge_dim_cppname: LAYER9_OUT_DIM\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"test_GNN\"\n",
    "config = config_from_pyg_model(torch_model,\n",
    "                                   default_precision=\"ap_fixed<32,12>\",\n",
    "                                   default_index_precision='ap_uint<16>', \n",
    "                                   default_reuse_factor=1)\n",
    "hls_model = convert_from_pyg_model(torch_model,\n",
    "                                       n_edge=graph_dims['n_edge'],\n",
    "                                       n_node=graph_dims['n_node'],\n",
    "                                       edge_attr=graph_dims['edge_attr'],\n",
    "                                       node_attr=graph_dims['node_attr'],\n",
    "                                       edge_dim=graph_dims['edge_dim'],\n",
    "                                       node_dim=graph_dims['node_dim'],\n",
    "                                       forward_dictionary=forward_dict, \n",
    "                                       activate_final='sigmoid',\n",
    "                                       output_dir=output_dir,\n",
    "                                       hls_config=config)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c4b1ec2",
   "metadata": {},
   "source": [
    "The user can also define different fixed-point precision, integer/index precision, or reuse-factor parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59aa1957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: {'Model': {'Precision': 'ap_fixed<32,12>', 'IndexPrecision': 'ap_uint<16>', 'ReuseFactor': 8, 'Strategy': 'Latency'}}\n",
      "PygModelReader node_dim: 5\n",
      "PygModelReader edge_dim: 5\n",
      "PygModelReader node_attr: 3\n",
      "PygModelReader edge_attr: 4\n",
      "node encoder layer_dict['n_in']: 3\n",
      "node encoder layer_dict['n_out']: 5\n",
      "edge encoder b4 update_dict[\"last_edge_update\"]: edge_attr\n",
      "edge encoder after update_dict[\"last_edge_update\"]: layer5_out\n",
      "EdgeBlock update_dict[\"last_edge_update\"] b4: layer5_out\n",
      "EdgeBlock update_dict[\"last_edge_update\"] after: layer6_out\n",
      "type(update_dict): <class 'dict'>\n",
      "layer_dict['inputs']: ['layer4_out', 'layer8_out']\n",
      "EdgeBlock update_dict[\"last_edge_update\"] b4: layer6_out\n",
      "EdgeBlock update_dict[\"last_edge_update\"] after: layer10_out\n",
      "NodeEncoder name: node_encoder\n",
      "self.state_dict.keys(): odict_keys(['R1.layers.0.weight', 'R1.layers.0.bias', 'R1.layers.2.weight', 'R1.layers.2.bias', 'R1.layers.4.weight', 'R1.layers.4.bias', 'O.layers.0.weight', 'O.layers.0.bias', 'O.layers.2.weight', 'O.layers.2.bias', 'O.layers.4.weight', 'O.layers.4.bias', 'R2.layers.0.weight', 'R2.layers.0.bias', 'R2.layers.2.weight', 'R2.layers.2.bias', 'R2.layers.4.weight', 'R2.layers.4.bias', 'node_encoder.weight', 'node_encoder.bias', 'edge_encoder.weight', 'edge_encoder.bias'])\n",
      "layer_name + '.' + var_name: node_encoder.weight\n",
      "self.state_dict.keys(): odict_keys(['R1.layers.0.weight', 'R1.layers.0.bias', 'R1.layers.2.weight', 'R1.layers.2.bias', 'R1.layers.4.weight', 'R1.layers.4.bias', 'O.layers.0.weight', 'O.layers.0.bias', 'O.layers.2.weight', 'O.layers.2.bias', 'O.layers.4.weight', 'O.layers.4.bias', 'R2.layers.0.weight', 'R2.layers.0.bias', 'R2.layers.2.weight', 'R2.layers.2.bias', 'R2.layers.4.weight', 'R2.layers.4.bias', 'node_encoder.weight', 'node_encoder.bias', 'edge_encoder.weight', 'edge_encoder.bias'])\n",
      "layer_name + '.' + var_name: node_encoder.bias\n",
      "EdgeEncoder name: edge_encoder\n",
      "self.state_dict.keys(): odict_keys(['R1.layers.0.weight', 'R1.layers.0.bias', 'R1.layers.2.weight', 'R1.layers.2.bias', 'R1.layers.4.weight', 'R1.layers.4.bias', 'O.layers.0.weight', 'O.layers.0.bias', 'O.layers.2.weight', 'O.layers.2.bias', 'O.layers.4.weight', 'O.layers.4.bias', 'R2.layers.0.weight', 'R2.layers.0.bias', 'R2.layers.2.weight', 'R2.layers.2.bias', 'R2.layers.4.weight', 'R2.layers.4.bias', 'node_encoder.weight', 'node_encoder.bias', 'edge_encoder.weight', 'edge_encoder.bias'])\n",
      "layer_name + '.' + var_name: edge_encoder.weight\n",
      "self.state_dict.keys(): odict_keys(['R1.layers.0.weight', 'R1.layers.0.bias', 'R1.layers.2.weight', 'R1.layers.2.bias', 'R1.layers.4.weight', 'R1.layers.4.bias', 'O.layers.0.weight', 'O.layers.0.bias', 'O.layers.2.weight', 'O.layers.2.bias', 'O.layers.4.weight', 'O.layers.4.bias', 'R2.layers.0.weight', 'R2.layers.0.bias', 'R2.layers.2.weight', 'R2.layers.2.bias', 'R2.layers.4.weight', 'R2.layers.4.bias', 'node_encoder.weight', 'node_encoder.bias', 'edge_encoder.weight', 'edge_encoder.bias'])\n",
      "layer_name + '.' + var_name: edge_encoder.bias\n",
      "self.attributes.keys(): dict_keys(['name', 'n_node', 'n_edge', 'node_dim', 'edge_dim', 'activate_final', 'activation', 'n_layers', 'out_dim', 'class_name', 'inputs', 'outputs', 'accum_t'])\n",
      "self.model.output_vars.keys(): dict_keys(['node_attr', 'edge_attr', 'edge_index', 'layer4_out', 'layer5_out'])\n",
      "self.edge_dim_cppname: N_LAYER_2_4\n",
      "self.attributes.keys(): dict_keys(['name', 'n_node', 'n_edge', 'node_dim', 'edge_dim', 'activate_final', 'activation', 'n_layers', 'out_dim', 'class_name', 'inputs', 'outputs', 'accum_t'])\n",
      "self.inputs[1]: layer7_out\n",
      "self.model.output_vars.keys(): dict_keys(['node_attr', 'edge_attr', 'edge_index', 'layer4_out', 'layer5_out', 'layer6_out', 'layer7_out'])\n",
      "self.edge_dim_cppname: N_LAYER_2_4\n",
      "rank: 2\n",
      "resblock name: aggr9\n",
      "self.attributes.keys(): dict_keys(['name', 'n_node', 'n_edge', 'node_dim', 'edge_dim', 'activate_final', 'activation', 'n_layers', 'out_dim', 'class_name', 'inputs', 'outputs', 'accum_t'])\n",
      "self.model.output_vars.keys(): dict_keys(['node_attr', 'edge_attr', 'edge_index', 'layer4_out', 'layer5_out', 'layer6_out', 'layer7_out', 'layer8_out', 'layer9_out'])\n",
      "self.edge_dim_cppname: LAYER9_OUT_DIM\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"test_GNN\"\n",
    "config = config_from_pyg_model(torch_model,\n",
    "                                   default_precision=\"ap_fixed<32,12>\",\n",
    "                                   default_index_precision='ap_uint<16>', \n",
    "                                   default_reuse_factor=8)\n",
    "print(f\"config: {config}\")\n",
    "hls_model = convert_from_pyg_model(torch_model,\n",
    "                                       n_edge=graph_dims['n_edge'],\n",
    "                                       n_node=graph_dims['n_node'],\n",
    "                                       edge_attr=graph_dims['edge_attr'],\n",
    "                                       node_attr=graph_dims['node_attr'],\n",
    "                                       edge_dim=graph_dims['edge_dim'],\n",
    "                                       node_dim=graph_dims['node_dim'],\n",
    "                                       forward_dictionary=forward_dict, \n",
    "                                       activate_final='sigmoid',\n",
    "                                       output_dir=output_dir,\n",
    "                                       hls_config=config)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5859accc",
   "metadata": {},
   "source": [
    "hls_model.compile() builds the C-function for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7da705cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing HLS project\n",
      "layer name: node_attr. func: None\n",
      "layer name: edge_attr. func: None\n",
      "layer name: edge_index. func: None\n",
      "layer name: node_encoder. func: ['nnet::dense<input_t, layer4_t, config4>(node_attr, layer4_out, w4, b4);']\n",
      "layer name: edge_encoder. func: ['nnet::dense<input2_t, layer5_t, config5>(edge_attr, layer5_out, w5, b5);']\n",
      "GraphBlock params['node_attr']: layer4_out\n",
      "GraphBlock params['edge_attr']: layer5_out\n",
      "layer name: R1. func: ['nnet::edgeblock<input2_t, input3_t, layer6_t, config6>(layer4_out, layer5_out, edge_index, layer6_out, R1_w0, R1_b0, R1_w1, R1_b1, R1_w2, R1_b2, R1_w3, R1_b3);']\n",
      "layer name: aggr7. func: ['nnet::edge_aggregate<input2_t, input3_t, layer7_t, aggregation_config7>(layer6_out, edge_index, layer7_out);']\n",
      "GraphBlock params['node_attr']: layer4_out\n",
      "GraphBlock params['edge_attr']: layer7_out\n",
      "nodeblock index: 8\n",
      "nodeblock _function_template: nnet::nodeblock<{input_t}, {output_t}, {config}>({node_attr}, {edge_attr_aggr}, {out}, {w0}, {b0}, {w1}, {b1}, {w2}, {b2}, {w3}, {b3});\n",
      "nodeblock final _function_template: nnet::nodeblock<input_t, layer8_t, config8>(layer4_out, layer7_out, layer8_out, O_w0, O_b0, O_w1, O_b1, O_w2, O_b2, O_w3, O_b3);\n",
      "layer name: O. func: ['nnet::nodeblock<input_t, layer8_t, config8>(layer4_out, layer7_out, layer8_out, O_w0, O_b0, O_w1, O_b1, O_w2, O_b2, O_w3, O_b3);']\n",
      "residualBlock template: nnet::{merge}<{input1_t}, {input2_t}, {output_t}, {config}>({input1}, {input2}, {output});\n",
      "final residualBlock template: nnet::residualBlock<layer4_t, layer8_t, layer9_t, config9>(layer4_out, layer8_out, layer9_out);\n",
      "layer name: aggr9. func: ['nnet::residualBlock<layer4_t, layer8_t, layer9_t, config9>(layer4_out, layer8_out, layer9_out);']\n",
      "GraphBlock params['node_attr']: layer9_out\n",
      "GraphBlock params['edge_attr']: layer6_out\n",
      "layer name: R2. func: ['nnet::edgeblock<input2_t, input3_t, layer10_t, config10>(layer9_out, layer6_out, edge_index, layer10_out, R2_w0, R2_b0, R2_w1, R2_b1, R2_w2, R2_b2, R2_w3, R2_b3);']\n",
      "layer name: final_act. func: ['nnet::sigmoid<layer10_t, result_t, sigmoid_config11>(layer10_out, layer11_out);']\n",
      "Dense config cpp\n",
      "Dense config cpp\n",
      "self.model.config.backend.config_templates.keys(): dict_keys(['Dense', 'BinaryDense', 'BatchNormalization', 'Conv1D', 'Conv2D', 'Conv2DBatchnorm', 'SeparableConv1D', 'SeparableConv2D', 'DepthwiseConv2D', 'Activation', 'BlockActivation', 'ParametrizedActivation', 'PReLU', 'Softmax', 'TernaryTanh', 'Pooling1D', 'Pooling2D', 'GlobalPooling1D', 'GlobalPooling2D', 'ZeroPadding1D', 'ZeroPadding2D', 'Merge', 'Concatenate', 'Dot', 'Resize', 'Transpose', 'GarNet', 'GarNetStack', 'EdgeBlock', 'NodeBlock', 'EdgeAggregate', 'ResidualBlock', 'NodeEncoder', 'EdgeEncoder', 'BatchNormalizationQuantizedTanh', 'PointwiseConv1D', 'PointwiseConv2D', 'Clone', 'CloneParallel', 'Repack', 'Broadcast', 'ApplyAlpha'])\n",
      "concat_config_template b4: struct config{index} : nnet::residual_config{{\n",
      "    static const unsigned n_elem = {n_elem};\n",
      "    static const bool gnn_resource_limit = {gnn_resource_limit};\n",
      "\n",
      "}};\n",
      "\n",
      "concat_config_template after: struct merge_config{index} : nnet::residual_config{{\n",
      "    static const unsigned n_elem = {n_elem};\n",
      "    static const bool gnn_resource_limit = {gnn_resource_limit};\n",
      "\n",
      "}};\n",
      "\n",
      "merge_config1_params: {'index': 1, 'n_elem': 'N_LAYER_2_4', 'gnn_resource_limit': 'false'}\n",
      "params['n_elem']: N_LAYER_1_4*N_LAYER_2_4\n",
      "GraphBlock params['node_attr']: layer4_out\n",
      "GraphBlock params['edge_attr']: layer5_out\n",
      "GraphBlock params['node_attr']: layer4_out\n",
      "GraphBlock params['edge_attr']: layer7_out\n",
      "nodeblock index: 8\n",
      "nodeblock _function_template: nnet::nodeblock<{input_t}, {output_t}, {config}>({node_attr}, {edge_attr_aggr}, {out}, {w0}, {b0}, {w1}, {b1}, {w2}, {b2}, {w3}, {b3});\n",
      "nodeblock final _function_template: nnet::nodeblock<input_t, layer8_t, config8>(layer4_out, layer7_out, layer8_out, O_w0, O_b0, O_w1, O_b1, O_w2, O_b2, O_w3, O_b3);\n",
      "residualBlock template: nnet::{merge}<{input1_t}, {input2_t}, {output_t}, {config}>({input1}, {input2}, {output});\n",
      "final residualBlock template: nnet::residualBlock<layer4_t, layer8_t, layer9_t, config9>(layer4_out, layer8_out, layer9_out);\n",
      "GraphBlock params['node_attr']: layer9_out\n",
      "GraphBlock params['edge_attr']: layer6_out\n",
      "Done\n",
      "lib_name: firmware/myproject-438e9BCD.so\n"
     ]
    }
   ],
   "source": [
    "hls_model.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e678b04",
   "metadata": {},
   "source": [
    "# Evaluation and prediction: hls_model.predict(input)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e31a4c5a",
   "metadata": {},
   "source": [
    "If your model takes a non-singular input (e.g. node attributes, edge attributes, and an edge index), then you should pass it as a list (e.g. [node_attr, edge_attr, edge_index]). See the \"data_wrapper\" class, and note that the hls_model.predict() method is used on the data.hls_data attribute. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661c3eaa",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df4856a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDS: [0 1 2 3 4 5 6 7 8 9]\n",
      "graphs length: 10\n",
      "writing test bench data for 1st graph\n"
     ]
    }
   ],
   "source": [
    "class data_wrapper(object):\n",
    "    def __init__(self, node_attr, edge_attr, edge_index, target):\n",
    "        self.x = node_attr\n",
    "        self.edge_attr = edge_attr\n",
    "        self.edge_index = edge_index.transpose(0,1)\n",
    "\n",
    "        node_attr, edge_attr, edge_index = self.x.detach().cpu().numpy(), self.edge_attr.detach().cpu().numpy(), self.edge_index.transpose(0, 1).detach().cpu().numpy().astype(np.float32)\n",
    "        node_attr, edge_attr, edge_index = np.ascontiguousarray(node_attr), np.ascontiguousarray(edge_attr), np.ascontiguousarray(edge_index)\n",
    "        self.hls_data = [node_attr, edge_attr, edge_index]\n",
    "\n",
    "        self.target = target\n",
    "        self.np_target = np.reshape(target.detach().cpu().numpy(), newshape=(target.shape[0],))\n",
    "\n",
    "def load_graphs(graph_indir, graph_dims, n_graphs):\n",
    "    graph_files = np.array(os.listdir(graph_indir))\n",
    "    graph_files = np.array([os.path.join(graph_indir, graph_file)\n",
    "                            for graph_file in graph_files])\n",
    "    n_graphs_total = len(graph_files)\n",
    "    IDs = np.arange(n_graphs_total)\n",
    "    print(f\"IDS: {IDs}\")\n",
    "    dataset = GraphDataset(graph_files=graph_files[IDs])\n",
    "\n",
    "    graphs = []\n",
    "    for data in dataset[:n_graphs]:\n",
    "        node_attr, edge_attr, edge_index, target, bad_graph = fix_graph_size(data.x, data.edge_attr, data.edge_index,\n",
    "                                                                             data.y,\n",
    "                                                                             n_node_max=graph_dims['n_node'],\n",
    "                                                                             n_edge_max=graph_dims['n_edge'])\n",
    "#         if not bad_graph:\n",
    "#             graphs.append(data_wrapper(node_attr, edge_attr, edge_index, target))\n",
    "        graphs.append(data_wrapper(node_attr, edge_attr, edge_index, target))\n",
    "    print(f\"graphs length: {len(graphs)}\")\n",
    "\n",
    "    print(\"writing test bench data for 1st graph\")\n",
    "    data = graphs[0]\n",
    "    node_attr, edge_attr, edge_index = data.x.detach().cpu().numpy(), data.edge_attr.detach().cpu().numpy(), data.edge_index.transpose(\n",
    "        0, 1).detach().cpu().numpy().astype(np.int32)\n",
    "    os.makedirs('tb_data', exist_ok=True)\n",
    "    input_data = np.concatenate([node_attr.reshape(1, -1), edge_attr.reshape(1, -1), edge_index.reshape(1, -1)], axis=1)\n",
    "    np.savetxt('tb_data/input_data.dat', input_data, fmt='%f', delimiter=' ')\n",
    "\n",
    "    return graphs\n",
    "\n",
    "\n",
    "graph_indir = \"trackml_data/processed_plus_pyg_small\"\n",
    "# graph_dims = {\n",
    "#         \"n_node\": 28,\n",
    "#         \"n_edge\": 37,\n",
    "#         \"node_dim\": 3,\n",
    "#         \"edge_dim\": 4\n",
    "#     }\n",
    "graphs = load_graphs(graph_indir, graph_dims, n_graphs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "374f8f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graphs len: 10\n",
      "<class 'list'>\n",
      "data.x shape:torch.Size([28, 3])\n",
      "torch pred shape: (37,)\n",
      "hls_pred.shape: (37,)\n",
      "MSE: 0.25778648257255554\n",
      "[0.5087067  0.49005595 0.5086075  0.5088681  0.51236296 0.51254725\n",
      " 0.51227176 0.5124683  0.48688036 0.51230854 0.5120531  0.5122293\n",
      " 0.50881284 0.50867295 0.4901742  0.50871056 0.48887783 0.5107962\n",
      " 0.510689   0.51081896 0.51073897 0.510098   0.5100721  0.50996697\n",
      " 0.510095   0.51001835 0.50944245 0.50950515 0.50938237 0.50952786\n",
      " 0.5094315  0.5088952  0.5083239  0.5081499  0.50739217 0.5082108\n",
      " 0.5081072 ]\n",
      "[0.49031672 0.49005595 0.4904159  0.4901553  0.4866605  0.48647615\n",
      " 0.4867517  0.48655516 0.48688036 0.4867149  0.48697037 0.48679414\n",
      " 0.4902106  0.49035048 0.4901742  0.49031287 0.48887783 0.48822728\n",
      " 0.4883344  0.48820445 0.48828447 0.4889255  0.4889513  0.4890565\n",
      " 0.48892844 0.48900506 0.489581   0.48951825 0.48964107 0.48949558\n",
      " 0.48959196 0.49012822 0.4906995  0.49087352 0.49065474 0.49081266\n",
      " 0.49091625]\n",
      "[0.99902344 0.         0.99902344 0.99902344 0.99902344 0.99902344\n",
      " 0.99902344 0.99902344 0.         0.99902344 0.99902344 0.99902344\n",
      " 0.99902344 0.99902344 0.         0.99902344 0.         0.99902344\n",
      " 0.99902344 0.99902344 0.99902344 0.99902344 0.99902344 0.99902344\n",
      " 0.99902344 0.99902344 0.99902344 0.99902344 0.99902344 0.99902344\n",
      " 0.99902344 0.99902344 0.99902344 0.99902344 0.9980469  0.99902344\n",
      " 0.99902344]\n"
     ]
    }
   ],
   "source": [
    "data = graphs[0]\n",
    "print(f\"graphs len: {len(graphs)}\")\n",
    "print(type(data.hls_data))\n",
    "print(f\"data.x shape:{data.x.shape}\")\n",
    "torch_pred = torch_model(data)\n",
    "torch_pred = torch_pred.detach().cpu().numpy().flatten()\n",
    "print(f\"torch pred shape: {torch_pred.shape}\")\n",
    "hls_pred = hls_model.predict(data.hls_data)\n",
    "print(f\"hls_pred.shape: {hls_pred.shape}\")\n",
    "MSE = mean_squared_error(torch_pred, hls_pred)\n",
    "print(f\"MSE: {MSE}\")\n",
    "\n",
    "print((np.abs(torch_pred- hls_pred)))\n",
    "print(torch_pred)\n",
    "print(hls_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63133890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meanNstd(data, torch_pred, hls_pred):\n",
    "#     print(data.x)\n",
    "    node_mean = torch.mean(data.x)\n",
    "    print(f\"node_mean: {node_mean}\")\n",
    "    node_std = torch.std(data.x)\n",
    "    print(f\"node_std: {node_std}\")\n",
    "    node_abs_max = torch.max(torch.abs(data.x))\n",
    "    print(f\"node_abs_max: {node_abs_max}\")\n",
    "    edge_mean = torch.mean(data.edge_attr)\n",
    "    print(f\"edge_mean: {edge_mean}\")\n",
    "    edge_std = torch.std(data.edge_attr)\n",
    "    print(f\"edge_std: {edge_std}\")\n",
    "    edge_abs_max = torch.max(torch.abs(data.edge_attr))\n",
    "    print(f\"edge_abs_max: {edge_abs_max}\")\n",
    "    torch_pred_mean = np.mean(torch_pred)\n",
    "    print(f\"torch_pred_mean: {torch_pred_mean}\")\n",
    "    torch_pred_std = np.std(torch_pred)\n",
    "    print(f\"torch_pred_std: {torch_pred_std}\")\n",
    "    torch_pred_abs_max = np.max(np.abs(torch_pred))\n",
    "    print(f\"torch_pred_abs_max: {torch_pred_abs_max}\")\n",
    "    hls_pred_mean = np.mean(hls_pred)\n",
    "    print(f\"hls_pred_mean: {hls_pred_mean}\")\n",
    "    hls_pred_std = np.std(hls_pred)\n",
    "    print(f\"hls_pred_std: {hls_pred_std}\")\n",
    "    hls_pred_abs_max = np.max(np.abs(hls_pred))\n",
    "    print(f\"hls_pred_abs_max: {hls_pred_abs_max}\")\n",
    "    diff_mean = np.mean(torch_pred- hls_pred)\n",
    "    print(f\"diff_mean: {diff_mean}\")\n",
    "    diff_std = np.std(torch_pred- hls_pred)\n",
    "    print(f\"diff_std: {diff_std}\")\n",
    "    diff_abs_max = np.max(np.abs(torch_pred- hls_pred))\n",
    "    print(f\"diff_abs_max: {diff_abs_max}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a300c466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall MSE: 0.24768313765525818\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "MSE_l = []\n",
    "for data in graphs:\n",
    "#     print(f\"graphs len: {len(graphs)}\")\n",
    "#     print(type(data.hls_data))\n",
    "#     print(f\"data.x shape:{data.x.shape}\")\n",
    "    torch_pred = torch_model(data)\n",
    "    torch_pred = torch_pred.detach().cpu().numpy().flatten()\n",
    "#     print(f\"torch pred shape: {torch_pred.shape}\")\n",
    "    hls_pred = hls_model.predict(data.hls_data)\n",
    "#     print(f\"hls_pred.shape: {hls_pred.shape}\")\n",
    "    MSE = mean_squared_error(torch_pred, hls_pred)\n",
    "#     get_meanNstd(data, torch_pred, hls_pred)\n",
    "#     print(f\"MSE: {MSE}\")\n",
    "    MSE_l.append(MSE)\n",
    "#     MAPE = mean_absolute_percentage_error(torch_pred, hls_pred)\n",
    "#     print(f\"MAPE: {MAPE}\")\n",
    "\n",
    "MSE_l = np.array(MSE_l)\n",
    "print(f\"overall MSE: {np.mean(MSE_l)}\")\n",
    "print(MSE_l.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b4fd437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 3)\n",
      "(37, 4)\n",
      "(37, 2)\n",
      "torch.Size([2, 37])\n"
     ]
    }
   ],
   "source": [
    "for data_instance in data.hls_data:\n",
    "    print(data_instance.shape)\n",
    "    \n",
    "print(data.edge_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e49ea99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class data_wrapper_tau3mu:\n",
    "#     def __init__(self, node_attr, edge_attr, edge_index, target):\n",
    "#         self.x = node_attr\n",
    "#         self.edge_attr = edge_attr\n",
    "#         self.edge_index = edge_index.transpose(0,1)\n",
    "\n",
    "#         node_attr, edge_attr, edge_index = self.x.detach().cpu().numpy(), self.edge_attr.detach().cpu().numpy(), self.edge_index.transpose(0, 1).detach().cpu().numpy().astype(np.float32)\n",
    "#         node_attr, edge_attr, edge_index = np.ascontiguousarray(node_attr), np.ascontiguousarray(edge_attr), np.ascontiguousarray(edge_index)\n",
    "#         self.hls_data = [node_attr, edge_attr, edge_index]\n",
    "\n",
    "#         self.target = target\n",
    "#         self.np_target = np.reshape(target.detach().cpu().numpy(), newshape=(target.shape[0],))\n",
    "\n",
    "def load_graphs_tau3mu(data_loader, graph_dims:dict, n_graphs =10000):\n",
    "    \"\"\"\n",
    "    params:\n",
    "    dataloader: pyg dataloader with custom Tau3MuDataset as its dataset\n",
    "    graph_dims: \n",
    "        graph_dims.keys = [\"n_node\": max number of nodes allowed in a graph/batch,\n",
    "            \"n_edge\": max number of edges allowed in a graph/batch,\n",
    "            \"node_dim\": feature dim of node,\n",
    "            \"edge_dim\": feature dim of edge\n",
    "        ]\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    graphs = []\n",
    "    \n",
    "    i = 0\n",
    "    for data in data_loader:\n",
    "        if i >= n_graphs:\n",
    "            break\n",
    "        data.edge_index = data.edge_index.t()#transpose the edge_index\n",
    "        n_edges = data.edge_attr.shape[0]\n",
    "#         print((data.y.shape))\n",
    "#         print(n_edges)\n",
    "        data.y = data.y.expand(1, n_edges).flatten()\n",
    "#         print(f\"data.y.shape: {data.y.shape}\")\n",
    "        node_attr, edge_attr, edge_index, target, bad_graph = fix_graph_size(data.x, \n",
    "                                                                             data.edge_attr, \n",
    "                                                                             data.edge_index,\n",
    "                                                                             data.y,\n",
    "                                                                             n_node_max=graph_dims['n_node'],\n",
    "                                                                             n_edge_max=graph_dims['n_edge'])\n",
    "        target = torch.flatten(target)# flatten target to 1d array\n",
    "        if not bad_graph:\n",
    "            graphs.append(data_wrapper(node_attr, edge_attr, edge_index, target))\n",
    "        i +=1\n",
    "        \n",
    "    print(f\"n_graphs: {len(graphs)}\")\n",
    "\n",
    "    print(\"writing test bench data for 1st graph\")\n",
    "    data = graphs[0]\n",
    "    node_attr, edge_attr, edge_index = data.x.detach().cpu().numpy(), data.edge_attr.detach().cpu().numpy(), data.edge_index.transpose(\n",
    "        0, 1).detach().cpu().numpy().astype(np.int32)\n",
    "    os.makedirs('tb_data', exist_ok=True)\n",
    "    input_data = np.concatenate([node_attr.reshape(1, -1), edge_attr.reshape(1, -1), edge_index.reshape(1, -1)], axis=1)\n",
    "    np.savetxt('tb_data/input_data.dat', input_data, fmt='%f', delimiter=' ')\n",
    "\n",
    "    return graphs\n",
    "\n",
    "\n",
    "graph_indir = \"trackml_data/processed_plus_pyg_small\"\n",
    "# graph_indir = \"/home/swissman777/projects/Tau3MuGNNs/data\"\n",
    "graph_dims = {\n",
    "        \"n_node\": 28,\n",
    "        \"n_edge\": 37,\n",
    "        \"node_dim\": 3,\n",
    "        \"edge_dim\": 4\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9b585b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': <torch_geometric.loader.dataloader.DataLoader object at 0x7f8c1804bf50>, 'valid': <torch_geometric.loader.dataloader.DataLoader object at 0x7f8c104c7d10>, 'test': <torch_geometric.loader.dataloader.DataLoader object at 0x7f8c104c77d0>}\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "from torch_geometric.loader.dataloader import DataLoader\n",
    "from utils.dataset import Tau3MuDataset\n",
    "\n",
    "# with open('./trackml_data/data_loaders.pickle', 'rb') as file:\n",
    "with open('./trackml_data/data_loaders_batch_size_1.pickle', 'rb') as file:\n",
    "    data_loaders= pkl.load(file) #, x_dim, edge_attr_dim \n",
    "\n",
    "print(data_loaders)\n",
    "# for stage in data_loaders.keys():\n",
    "#     for data in data_loaders[stage]:\n",
    "#         print(data.x.shape)\n",
    "#         print(data.edge_index.shape)\n",
    "#         print(data.edge_attr.shape)\n",
    "\n",
    "# print(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62330a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_graphs: 10000\n",
      "writing test bench data for 1st graph\n",
      "n_graphs: 10000\n",
      "writing test bench data for 1st graph\n",
      "n_graphs: 10000\n",
      "writing test bench data for 1st graph\n",
      "MSE means: 0.09450667351484299\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "MSE_l = []\n",
    "stages = [\"train\", \"valid\", \"test\"]\n",
    "for stage in stages:\n",
    "    graphs = load_graphs_tau3mu(data_loaders[stage], graph_dims)\n",
    "    for data in graphs:\n",
    "        torch_pred = torch_model(data)\n",
    "        torch_pred = torch_pred.detach().cpu().numpy()\n",
    "        hls_pred = hls_model.predict(data.hls_data)\n",
    "        MSE = mean_squared_error(torch_pred, hls_pred)\n",
    "    #     get_meanNstd(data,torch_pred, hls_pred)\n",
    "    #     print(f\"MSE: {MSE}\")\n",
    "        MSE_l.append(MSE)\n",
    "    #     MAPE = mean_absolute_percentage_error(torch_pred, hls_pred)\n",
    "    #     print(f\"MAPE: {MAPE}\")\n",
    "MSE_l = np.array(MSE_l)\n",
    "print(f\"MSE means: {np.mean(MSE_l)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fe5a646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5741bccd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
