{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1bc2776",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9b384d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handler args: ('NodeBlock',)\n",
      "handler args: ('EdgeBlock',)\n",
      "handler args: ('EdgeAggregate',)\n",
      "handler args: ('ResidualBlock',)\n",
      "handler args: ('NodeEncoder',)\n",
      "handler args: ('EdgeEncoder',)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from hls4ml.utils.config import config_from_pyg_model\n",
    "from hls4ml.converters import convert_from_pyg_model\n",
    "# module_path = os.path.abspath(os.path.join('../pyg_to_hls_hls4ml/hls4ml/utils'))\n",
    "# print(module_path)\n",
    "# if module_path not in sys.path:\n",
    "#     sys.path.append(module_path)\n",
    "# from config import config_from_pyg_model\n",
    "\n",
    "# module_path = os.path.abspath(os.path.join('../pyg_to_hls_hls4ml/hls4ml'))\n",
    "# print(module_path)\n",
    "# if module_path not in sys.path:\n",
    "#     sys.path.append(module_path)\n",
    "# from converters import convert_from_pyg_model\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# locals\n",
    "from utils.models.interaction_network_pyg import InteractionNetwork\n",
    "from model_wrappers import model_wrapper\n",
    "from utils.data.dataset_pyg import GraphDataset\n",
    "from utils.data.fix_graph_size import fix_graph_size\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb118bac",
   "metadata": {},
   "source": [
    "### PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24082fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([0.14819485, 0.10776299, 0.18915886, 0.07321286, 0.08802354, 0.95936340,\n",
      "        0.32395267, 0.09556627, 0.99297142, 0.97439438, 0.52161252, 0.19358766,\n",
      "        0.66018587, 0.44498897, 0.60995883, 0.10150993, 0.45451957, 0.52531058,\n",
      "        0.93877780, 0.28106594, 0.14415997, 0.29765218, 0.96479279, 0.84275991,\n",
      "        0.47767246, 0.47202319, 0.24503112, 0.77267098, 0.72283816, 0.95040357,\n",
      "        0.48314852, 0.29976338, 0.51107556, 0.39307076, 0.49783385, 0.69130814,\n",
      "        0.25721705, 0.40953445, 0.73893833, 0.36656791])), ('bias', tensor([0.26053399, 0.07725668, 0.47410673, 0.22577035, 0.87139207, 0.39811850,\n",
      "        0.86413956, 0.69497591, 0.05985016, 0.87185097, 0.53687239, 0.02068603,\n",
      "        0.82220358, 0.16684115, 0.60327083, 0.40372521, 0.86406064, 0.62994492,\n",
      "        0.24517769, 0.38250852, 0.09975529, 0.67661357, 0.23945385, 0.28818518,\n",
      "        0.00484180, 0.87734479, 0.35381472, 0.77792144, 0.49775392, 0.64469302,\n",
      "        0.29402006, 0.23904103, 0.94205368, 0.91451412, 0.19720578, 0.70901442,\n",
      "        0.95865119, 0.98646808, 0.53752917, 0.74956745])), ('running_mean', tensor([0.23881114, 0.76487070, 0.36272603, 0.21982616, 0.75213790, 0.87497890,\n",
      "        0.99286741, 0.92081767, 0.21496528, 0.36621797, 0.03542006, 0.49738544,\n",
      "        0.00384820, 0.09175795, 0.88197589, 0.22349364, 0.66216993, 0.66571623,\n",
      "        0.83645523, 0.98348230, 0.64807934, 0.85788292, 0.05898660, 0.68605804,\n",
      "        0.74336100, 0.36363852, 0.62616783, 0.84835619, 0.14648753, 0.35127932,\n",
      "        0.22986609, 0.24185967, 0.20556456, 0.05409336, 0.79315257, 0.06770211,\n",
      "        0.70531595, 0.18319172, 0.70889091, 0.84809911])), ('running_var', tensor([0.74169284, 0.29528672, 0.94728231, 0.18485034, 0.86664104, 0.05316299,\n",
      "        0.10239923, 0.53791696, 0.88624364, 0.15851945, 0.01221406, 0.55174136,\n",
      "        0.13804513, 0.35408777, 0.74860764, 0.00642866, 0.34317511, 0.40417963,\n",
      "        0.33606440, 0.13072062, 0.91768616, 0.79321128, 0.56834692, 0.51439631,\n",
      "        0.56689405, 0.56685263, 0.25548136, 0.38211977, 0.39344883, 0.72810948,\n",
      "        0.76542497, 0.67342502, 0.80724776, 0.54639745, 0.44885993, 0.10305703,\n",
      "        0.49336278, 0.23582417, 0.71654379, 0.86251473])), ('num_batches_tracked', tensor(0))])\n"
     ]
    }
   ],
   "source": [
    "torch_model = InteractionNetwork(aggr=\"add\", flow=\"source_to_target\", hidden_size=40).eval()\n",
    "# torch_model_dict = torch.load(\"trained_models//IN_pyg_small_add_source_to_target_40_state_dict.pt\")\n",
    "# torch_model.load_state_dict(torch_model_dict)\n",
    "new_state_dict = OrderedDict()\n",
    "for key in torch_model.O.layers[1].state_dict():\n",
    "#     print(torch_model.O.layers[1].state_dict()[key].shape)\n",
    "    if key != \"num_batches_tracked\":\n",
    "        new_state_dict[key] = torch.rand(40)\n",
    "\n",
    "torch_model.O.layers[1].load_state_dict(new_state_dict)\n",
    "print(torch_model.O.layers[1].state_dict())\n",
    "# for name, submodule in torch_model.named_modules():\n",
    "#     if name != \"\":\n",
    "#         print(f\"{name}: {submodule}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2293596d",
   "metadata": {},
   "source": [
    "We can see that this specific GNN is composed of 3 submodules:\n",
    "- The first submodule, \"R1\", is a \"RelationalModel\" a.k.a. an \"EdgeBlock\"\n",
    "- The second submodule, \"O\", is an \"ObjectModel\" a.k.a. a \"NodeBlock\"\n",
    "- The third submodule, \"R2\" is another \"RelationalModel\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e49e754",
   "metadata": {},
   "source": [
    "### HLS Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e798bed7",
   "metadata": {},
   "source": [
    "hls4ml cannot infer the *order* in which these submodules are called within the pytorch model's \"forward()\" function. We have to manually define this information in the form of an ordered-dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18b64da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward_dict: defines the order in which graph-blocks are called in the model's 'forward()' method\n",
    "forward_dict = OrderedDict()\n",
    "forward_dict[\"node_encoder\"] = \"NodeEncoder\"\n",
    "forward_dict[\"edge_encoder\"] = \"EdgeEncoder\"\n",
    "# forward_dict[\"R1\"] = \"EdgeBlock\"\n",
    "forward_dict[\"O\"] = \"NodeBlock\"\n",
    "# forward_dict[\"res_block\"] = \"ResidualBlock\"\n",
    "# forward_dict[\"R2\"] = \"EdgeBlock\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "6dca8d9f",
   "metadata": {},
   "source": [
    "hls4ml creates a hardware implementation of the GNN, which can only be represented using fixed-size arrays. This restriction also applies to the inputs and outputs of the GNN, so we must define the size of the graphs that this hardware GNN can take as input**, again in the form of a dictionary. \n",
    "\n",
    "**Graphs of a different size can be padded or truncated to the appropriate size using the \"fix_graph_size\" function. In this notebook, padding/truncation is  done in the \"Data\" cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baa5a9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_dim = 5\n",
    "graph_dims = {\n",
    "        \"n_node\": 28,\n",
    "        \"n_edge\": 37,\n",
    "        \"node_attr\": 3,\n",
    "        \"node_dim\": common_dim,\n",
    "        \"edge_attr\": 4,\n",
    "    \"edge_dim\":common_dim\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "623f2192",
   "metadata": {},
   "source": [
    "Armed with our pytorch model and these two dictionaries**, we can create the HLS model. \n",
    "\n",
    "**If there is some activation function after the output of the final GNN-submodule, we also have to pass the type of this activation through the \"activate_final\" parameter of \"convert_from_pyg_model\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da3c3fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir = \"test_GNN\"\n",
    "# config = config_from_pyg_model(torch_model,\n",
    "#                                    default_precision=\"ap_fixed<32,12>\",\n",
    "#                                    default_index_precision='ap_uint<16>', \n",
    "#                                    default_reuse_factor=1)\n",
    "# hls_model = convert_from_pyg_model(torch_model,\n",
    "#                                        n_edge=graph_dims['n_edge'],\n",
    "#                                        n_node=graph_dims['n_node'],\n",
    "#                                        edge_attr=graph_dims['edge_attr'],\n",
    "#                                        node_attr=graph_dims['node_attr'],\n",
    "#                                        edge_dim=graph_dims['edge_dim'],\n",
    "#                                        node_dim=graph_dims['node_dim'],\n",
    "#                                        forward_dictionary=forward_dict, \n",
    "#                                        activate_final='sigmoid',\n",
    "#                                        output_dir=output_dir,\n",
    "#                                        hls_config=config)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c4b1ec2",
   "metadata": {},
   "source": [
    "The user can also define different fixed-point precision, integer/index precision, or reuse-factor parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59aa1957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: {'Model': {'Precision': 'ap_fixed<52,20>', 'IndexPrecision': 'ap_uint<16>', 'ReuseFactor': 8, 'Strategy': 'Latency'}}\n",
      "pyg_to_hls config: {'OutputDir': 'test_GNN', 'ProjectName': 'myproject', 'Backend': 'Vivado', 'XilinxPart': 'xcku115-flvb2104-2-i', 'Board': None, 'ClockPeriod': 5, 'IOType': 'io_parallel', 'HLSConfig': {'Model': {'Precision': 'ap_fixed<52,20>', 'IndexPrecision': 'ap_uint<16>', 'ReuseFactor': 8, 'Strategy': 'Latency'}}, 'PytorchModel': InteractionNetwork(), 'InputShape': {'NodeAttr': [28, 3], 'EdgeAttr': [37, 4], 'EdgeIndex': [37, 2], 'NodeDim': 5, 'EdgeDim': 5}, 'ForwardDictionary': OrderedDict([('node_encoder', 'NodeEncoder'), ('edge_encoder', 'EdgeEncoder'), ('O', 'NodeBlock')]), 'ActivateFinal': 'sigmoid', 'ParallelizationFactor': 16, 'gnn_resource_limit': 'false'}\n",
      "PygModelReader node_dim: 5\n",
      "PygModelReader edge_dim: 5\n",
      "PygModelReader node_attr: 3\n",
      "PygModelReader edge_attr: 4\n",
      "node encoder layer_dict['n_in']: 3\n",
      "node encoder layer_dict['n_out']: 5\n",
      "node_encoder layer_dict: {'name': 'node_encoder', 'class_name': 'NodeEncoder', 'n_in': 3, 'n_out': 5, 'n_rows': 28, 'n_cols': 3, 'inputs': ['node_attr'], 'outputs': ['layer4_out'], 'activate_final': 'false'}\n",
      "edge encoder b4 update_dict[\"last_edge_update\"]: edge_attr\n",
      "edge encoder after update_dict[\"last_edge_update\"]: layer5_out\n",
      "edge_encoder layer_dict: {'name': 'edge_encoder', 'class_name': 'EdgeEncoder', 'n_in': 4, 'n_out': 5, 'n_rows': 37, 'n_cols': 4, 'inputs': ['edge_attr'], 'outputs': ['layer5_out'], 'activate_final': 'false'}\n",
      "aggr1 layer_dict: {'name': 'aggr6', 'class_name': 'EdgeAggregate', 'n_node': 28, 'n_edge': 37, 'node_dim': 5, 'edge_dim': 5, 'out_dim': 5, 'inputs': ['layer4_out', 'layer5_out', 'edge_index'], 'outputs': ['layer6_out'], 'activate_final': 'false'}\n",
      "type(update_dict): <class 'dict'>\n",
      "nodeblock layer_dict: {'name': 'O', 'n_node': 28, 'n_edge': 37, 'node_dim': 5, 'edge_dim': 5, 'activate_final': 'false', 'activation': 'linear', 'n_layers': 2, 'out_dim': 5, 'class_name': 'NodeBlock', 'inputs': ['layer4_out', 'layer6_out'], 'outputs': ['layer7_out']}\n",
      "O layer_dict: {'name': 'O', 'n_node': 28, 'n_edge': 37, 'node_dim': 5, 'edge_dim': 5, 'activate_final': 'false', 'activation': 'linear', 'n_layers': 2, 'out_dim': 5, 'class_name': 'NodeBlock', 'inputs': ['layer4_out', 'layer6_out'], 'outputs': ['layer7_out']}\n",
      "NodeEncoder name: node_encoder\n",
      "self.inputs[1]: layer6_out\n",
      "module.__class__.__name__: Sequential\n",
      "module.__class__.__name__: Linear\n",
      "add_weights Linear activated\n",
      "module.__class__.__name__: BatchNorm1d\n",
      "add_weights BatchNorm1d activated\n",
      "O.layers.1\n",
      "module.__class__.__name__: LeakyReLU\n",
      "module.__class__.__name__: Linear\n",
      "add_weights Linear activated\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"test_GNN\"\n",
    "config = config_from_pyg_model(torch_model,\n",
    "                                   default_precision=\"ap_fixed<52,20>\",\n",
    "                                   default_index_precision='ap_uint<16>', \n",
    "                                   default_reuse_factor=8)\n",
    "print(f\"config: {config}\")\n",
    "hls_model = convert_from_pyg_model(torch_model,\n",
    "                                       n_edge=graph_dims['n_edge'],\n",
    "                                       n_node=graph_dims['n_node'],\n",
    "                                       edge_attr=graph_dims['edge_attr'],\n",
    "                                       node_attr=graph_dims['node_attr'],\n",
    "                                       edge_dim=graph_dims['edge_dim'],\n",
    "                                       node_dim=graph_dims['node_dim'],\n",
    "                                       forward_dictionary=forward_dict, \n",
    "                                       activate_final='sigmoid', #sigmoid\n",
    "                                       output_dir=output_dir,\n",
    "                                       hls_config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f0701c",
   "metadata": {},
   "source": [
    "## hls_model.compile() builds the C-function for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7da705cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing HLS project\n",
      "def_cpp: layer4_t layer4_out[N_LAYER_1_4*N_LAYER_2_4]\n",
      "def_cpp: layer5_t layer5_out[N_LAYER_1_5*N_LAYER_2_5]\n",
      "def_cpp: layer6_t layer6_out[N_NODE*LAYER6_OUT_DIM]\n",
      "def_cpp: layer7_t layer7_out[N_LAYER_1_4*LAYER7_OUT_DIM]\n",
      "Done\n",
      "lib_name: firmware/myproject-A43ecFd9.so\n"
     ]
    }
   ],
   "source": [
    "hls_model.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e678b04",
   "metadata": {},
   "source": [
    "# Evaluation and prediction: hls_model.predict(input)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e31a4c5a",
   "metadata": {},
   "source": [
    "If your model takes a non-singular input (e.g. node attributes, edge attributes, and an edge index), then you should pass it as a list (e.g. [node_attr, edge_attr, edge_index]). See the \"data_wrapper\" class, and note that the hls_model.predict() method is used on the data.hls_data attribute. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661c3eaa",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df4856a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDS: [0 1 2 3 4 5 6 7 8 9]\n",
      "graphs length: 10\n",
      "writing test bench data for 1st graph\n"
     ]
    }
   ],
   "source": [
    "class data_wrapper(object):\n",
    "    def __init__(self, node_attr, edge_attr, edge_index, target):\n",
    "        self.x = node_attr\n",
    "        self.edge_attr = edge_attr\n",
    "        self.edge_index = edge_index.transpose(0,1)\n",
    "\n",
    "        node_attr, edge_attr, edge_index = self.x.detach().cpu().numpy(), self.edge_attr.detach().cpu().numpy(), self.edge_index.transpose(0, 1).detach().cpu().numpy().astype(np.float32)\n",
    "        node_attr, edge_attr, edge_index = np.ascontiguousarray(node_attr), np.ascontiguousarray(edge_attr), np.ascontiguousarray(edge_index)\n",
    "        self.hls_data = [node_attr, edge_attr, edge_index]\n",
    "\n",
    "        self.target = target\n",
    "        self.np_target = np.reshape(target.detach().cpu().numpy(), newshape=(target.shape[0],))\n",
    "\n",
    "def load_graphs(graph_indir, graph_dims, n_graphs):\n",
    "    graph_files = np.array(os.listdir(graph_indir))\n",
    "    graph_files = np.array([os.path.join(graph_indir, graph_file)\n",
    "                            for graph_file in graph_files])\n",
    "    n_graphs_total = len(graph_files)\n",
    "    IDs = np.arange(n_graphs_total)\n",
    "    print(f\"IDS: {IDs}\")\n",
    "    dataset = GraphDataset(graph_files=graph_files[IDs])\n",
    "\n",
    "    graphs = []\n",
    "    for data in dataset[:n_graphs]:\n",
    "        node_attr, edge_attr, edge_index, target, bad_graph = fix_graph_size(data.x, data.edge_attr, data.edge_index,\n",
    "                                                                             data.y,\n",
    "                                                                             n_node_max=graph_dims['n_node'],\n",
    "                                                                             n_edge_max=graph_dims['n_edge'])\n",
    "#         if not bad_graph:\n",
    "#             graphs.append(data_wrapper(node_attr, edge_attr, edge_index, target))\n",
    "        graphs.append(data_wrapper(node_attr, edge_attr, edge_index, target))\n",
    "    print(f\"graphs length: {len(graphs)}\")\n",
    "\n",
    "    print(\"writing test bench data for 1st graph\")\n",
    "    data = graphs[0]\n",
    "    node_attr, edge_attr, edge_index = data.x.detach().cpu().numpy(), data.edge_attr.detach().cpu().numpy(), data.edge_index.transpose(\n",
    "        0, 1).detach().cpu().numpy().astype(np.int32)\n",
    "    os.makedirs('tb_data', exist_ok=True)\n",
    "    input_data = np.concatenate([node_attr.reshape(1, -1), edge_attr.reshape(1, -1), edge_index.reshape(1, -1)], axis=1)\n",
    "    np.savetxt('tb_data/input_data.dat', input_data, fmt='%f', delimiter=' ')\n",
    "\n",
    "    return graphs\n",
    "\n",
    "\n",
    "graph_indir = \"trackml_data/processed_plus_pyg_small\"\n",
    "# graph_dims = {\n",
    "#         \"n_node\": 28,\n",
    "#         \"n_edge\": 37,\n",
    "#         \"node_dim\": 3,\n",
    "#         \"edge_dim\": 4\n",
    "#     }\n",
    "graphs = load_graphs(graph_indir, graph_dims, n_graphs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "374f8f4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "hls_pred.shape: (140,)\n",
      "MSE: 4.562617505143862e-06\n"
     ]
    }
   ],
   "source": [
    "data = graphs[0]\n",
    "# print(f\"graphs len: {len(graphs)}\")\n",
    "print(type(data.hls_data))\n",
    "# print(f\"data.x shape:{data.x.shape}\")\n",
    "torch_pred = torch_model(data)\n",
    "torch_pred = torch_pred.detach().cpu().numpy().flatten()\n",
    "# print(f\"torch pred shape: {torch_pred.shape}\")\n",
    "hls_pred = hls_model.predict(data.hls_data)\n",
    "time.sleep(14)\n",
    "print(f\"hls_pred.shape: {hls_pred.shape}\")\n",
    "MSE = mean_squared_error(torch_pred, hls_pred)\n",
    "print(f\"MSE: {MSE}\")\n",
    "# print((np.abs(torch_pred- hls_pred)))\n",
    "# print(torch_pred)\n",
    "# print(hls_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63133890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meanNstd(data, torch_pred, hls_pred):\n",
    "#     print(data.x)\n",
    "    node_mean = torch.mean(data.x)\n",
    "    print(f\"node_mean: {node_mean}\")\n",
    "    node_std = torch.std(data.x)\n",
    "    print(f\"node_std: {node_std}\")\n",
    "    node_abs_max = torch.max(torch.abs(data.x))\n",
    "    print(f\"node_abs_max: {node_abs_max}\")\n",
    "    edge_mean = torch.mean(data.edge_attr)\n",
    "    print(f\"edge_mean: {edge_mean}\")\n",
    "    edge_std = torch.std(data.edge_attr)\n",
    "    print(f\"edge_std: {edge_std}\")\n",
    "    edge_abs_max = torch.max(torch.abs(data.edge_attr))\n",
    "    print(f\"edge_abs_max: {edge_abs_max}\")\n",
    "    torch_pred_mean = np.mean(torch_pred)\n",
    "    print(f\"torch_pred_mean: {torch_pred_mean}\")\n",
    "    torch_pred_std = np.std(torch_pred)\n",
    "    print(f\"torch_pred_std: {torch_pred_std}\")\n",
    "    torch_pred_abs_max = np.max(np.abs(torch_pred))\n",
    "    print(f\"torch_pred_abs_max: {torch_pred_abs_max}\")\n",
    "    hls_pred_mean = np.mean(hls_pred)\n",
    "    print(f\"hls_pred_mean: {hls_pred_mean}\")\n",
    "    hls_pred_std = np.std(hls_pred)\n",
    "    print(f\"hls_pred_std: {hls_pred_std}\")\n",
    "    hls_pred_abs_max = np.max(np.abs(hls_pred))\n",
    "    print(f\"hls_pred_abs_max: {hls_pred_abs_max}\")\n",
    "    diff_mean = np.mean(torch_pred- hls_pred)\n",
    "    print(f\"diff_mean: {diff_mean}\")\n",
    "    diff_std = np.std(torch_pred- hls_pred)\n",
    "    print(f\"diff_std: {diff_std}\")\n",
    "    diff_abs_max = np.max(np.abs(torch_pred- hls_pred))\n",
    "    print(f\"diff_abs_max: {diff_abs_max}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a300c466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 4.562617505143862e-06\n",
      "MSE: 4.4236076064407825e-06\n",
      "MSE: 1.6629373931209557e-05\n",
      "MSE: 4.64712647954002e-05\n",
      "MSE: 4.704269940702943e-06\n",
      "MSE: 4.216642992105335e-06\n",
      "MSE: 4.249519861332374e-06\n",
      "MSE: 2.8564481908688322e-05\n",
      "MSE: 4.582159363053506e-06\n",
      "MSE: 4.726725364889717e-06\n",
      "MSE_l: [4.5626175e-06 4.4236076e-06 1.6629374e-05 4.6471265e-05 4.7042699e-06\n",
      " 4.2166430e-06 4.2495199e-06 2.8564482e-05 4.5821594e-06 4.7267254e-06]\n",
      "overall MSE: 1.231306669069454e-05\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "MSE_l = []\n",
    "for data in graphs:\n",
    "#     print(f\"graphs len: {len(graphs)}\")\n",
    "#     print(type(data.hls_data))\n",
    "#     print(f\"data.x shape:{data.x.shape}\")\n",
    "    torch_pred = torch_model(data)\n",
    "    torch_pred = torch_pred.detach().cpu().numpy().flatten()\n",
    "#     print(f\"torch pred shape: {torch_pred.shape}\")\n",
    "    hls_pred = hls_model.predict(data.hls_data)\n",
    "#     print(f\"hls_pred.shape: {hls_pred.shape}\")\n",
    "    MSE = mean_squared_error(torch_pred, hls_pred)\n",
    "#     get_meanNstd(data, torch_pred, hls_pred)\n",
    "    print(f\"MSE: {MSE}\")\n",
    "    MSE_l.append(MSE)\n",
    "#     MAPE = mean_absolute_percentage_error(torch_pred, hls_pred)\n",
    "#     print(f\"MAPE: {MAPE}\")\n",
    "\n",
    "MSE_l = np.array(MSE_l)\n",
    "print(f\"MSE_l: {MSE_l}\")\n",
    "print(f\"overall MSE: {np.mean(MSE_l)}\")\n",
    "print(MSE_l.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b4fd437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 3)\n",
      "(37, 4)\n",
      "(37, 2)\n",
      "torch.Size([2, 37])\n"
     ]
    }
   ],
   "source": [
    "for data_instance in data.hls_data:\n",
    "    print(data_instance.shape)\n",
    "    \n",
    "print(data.edge_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e49ea99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class data_wrapper_tau3mu:\n",
    "#     def __init__(self, node_attr, edge_attr, edge_index, target):\n",
    "#         self.x = node_attr\n",
    "#         self.edge_attr = edge_attr\n",
    "#         self.edge_index = edge_index.transpose(0,1)\n",
    "\n",
    "#         node_attr, edge_attr, edge_index = self.x.detach().cpu().numpy(), self.edge_attr.detach().cpu().numpy(), self.edge_index.transpose(0, 1).detach().cpu().numpy().astype(np.float32)\n",
    "#         node_attr, edge_attr, edge_index = np.ascontiguousarray(node_attr), np.ascontiguousarray(edge_attr), np.ascontiguousarray(edge_index)\n",
    "#         self.hls_data = [node_attr, edge_attr, edge_index]\n",
    "\n",
    "#         self.target = target\n",
    "#         self.np_target = np.reshape(target.detach().cpu().numpy(), newshape=(target.shape[0],))\n",
    "\n",
    "def load_graphs_tau3mu(data_loader, graph_dims:dict, n_graphs =100000000): #n_graphs =1000\n",
    "    \"\"\"\n",
    "    params:\n",
    "    dataloader: pyg dataloader with custom Tau3MuDataset as its dataset\n",
    "    graph_dims: \n",
    "        graph_dims.keys = [\"n_node\": max number of nodes allowed in a graph/batch,\n",
    "            \"n_edge\": max number of edges allowed in a graph/batch,\n",
    "            \"node_dim\": feature dim of node,\n",
    "            \"edge_dim\": feature dim of edge\n",
    "        ]\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    graphs = []\n",
    "    \n",
    "    i = 0\n",
    "    for data in data_loader:\n",
    "        if i >= n_graphs:\n",
    "            break\n",
    "        data.edge_index = data.edge_index.t()#transpose the edge_index\n",
    "        n_edges = data.edge_attr.shape[0]\n",
    "#         print((data.y.shape))\n",
    "#         print(n_edges)\n",
    "        data.y = data.y.expand(1, n_edges).flatten()\n",
    "#         print(f\"data.y.shape: {data.y.shape}\")\n",
    "        node_attr, edge_attr, edge_index, target, bad_graph = fix_graph_size(data.x, \n",
    "                                                                             data.edge_attr, \n",
    "                                                                             data.edge_index,\n",
    "                                                                             data.y,\n",
    "                                                                             n_node_max=graph_dims['n_node'],\n",
    "                                                                             n_edge_max=graph_dims['n_edge'])\n",
    "        target = torch.flatten(target)# flatten target to 1d array\n",
    "\n",
    "        if not bad_graph:\n",
    "            graphs.append(data_wrapper(node_attr, edge_attr, edge_index, target))\n",
    "        else:\n",
    "            print(\"bad graph\")\n",
    "        i +=1\n",
    "        \n",
    "    print(f\"n_graphs: {len(graphs)}\")\n",
    "\n",
    "    print(\"writing test bench data for 1st graph\")\n",
    "    data = graphs[0]\n",
    "    node_attr, edge_attr, edge_index = data.x.detach().cpu().numpy(), data.edge_attr.detach().cpu().numpy(), data.edge_index.transpose(\n",
    "        0, 1).detach().cpu().numpy().astype(np.int32)\n",
    "    os.makedirs('tb_data', exist_ok=True)\n",
    "    input_data = np.concatenate([node_attr.reshape(1, -1), edge_attr.reshape(1, -1), edge_index.reshape(1, -1)], axis=1)\n",
    "    np.savetxt('tb_data/input_data.dat', input_data, fmt='%f', delimiter=' ')\n",
    "\n",
    "    return graphs\n",
    "\n",
    "\n",
    "graph_indir = \"trackml_data/processed_plus_pyg_small\"\n",
    "# graph_indir = \"/home/swissman777/projects/Tau3MuGNNs/data\"\n",
    "graph_dims = {\n",
    "        \"n_node\": 28,\n",
    "        \"n_edge\": 37,\n",
    "        \"node_dim\": 3,\n",
    "        \"edge_dim\": 4\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9b585b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': <torch_geometric.loader.dataloader.DataLoader object at 0x7fda8ceba290>, 'valid': <torch_geometric.loader.dataloader.DataLoader object at 0x7fda8cd5f490>, 'test': <torch_geometric.loader.dataloader.DataLoader object at 0x7fda8cd5f510>}\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "from torch_geometric.loader.dataloader import DataLoader\n",
    "from utils.dataset import Tau3MuDataset\n",
    "\n",
    "# with open('./trackml_data/data_loaders.pickle', 'rb') as file:\n",
    "with open('./trackml_data/data_loaders_batch_size_1.pickle', 'rb') as file:\n",
    "    data_loaders= pkl.load(file) #, x_dim, edge_attr_dim \n",
    "\n",
    "print(data_loaders)\n",
    "# for stage in data_loaders.keys():\n",
    "#     for data in data_loaders[stage]:\n",
    "#         print(data.x.shape)\n",
    "#         print(data.edge_index.shape)\n",
    "#         print(data.edge_attr.shape)\n",
    "\n",
    "# print(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62330a77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_graphs: 332406\n",
      "writing test bench data for 1st graph\n",
      "n_graphs: 71226\n",
      "writing test bench data for 1st graph\n",
      "n_graphs: 175113\n",
      "writing test bench data for 1st graph\n",
      "MSE_l :[6.0180295e-03 3.0593721e-03 3.5855030e-06 ... 6.7110254e-06 2.5671576e-05\n",
      " 4.3877955e-03]\n",
      "MSE means: 0.0053800009191036224\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "MSE_l = []\n",
    "stages = [\"train\", \"valid\", \"test\"]\n",
    "# node_scaler = StandardScaler()\n",
    "# edge_scaler = StandardScaler()\n",
    "# for stage in [stages[0]]:\n",
    "for stage in stages:\n",
    "    graphs = load_graphs_tau3mu(data_loaders[stage], graph_dims)\n",
    "#     print(type(graphs))\n",
    "\n",
    "#     for data in graphs: # fit data to scaler b4 feedforwarding\n",
    "# #         print(f\"data type: {type(data)}\")\n",
    "#         node_scaler.fit(data.x) \n",
    "#         edge_scaler.fit(data.edge_attr) \n",
    "    \n",
    "    for data in graphs:\n",
    "       \n",
    "#         data.x = torch.from_numpy(node_scaler.transform(data.x)).float()\n",
    "#         data.edge_attr = torch.from_numpy(edge_scaler.transform(data.edge_attr)).float()\n",
    "#         data.hls_data[0] = node_scaler.transform(data.hls_data[0])\n",
    "#         data.hls_data[1] = edge_scaler.transform(data.hls_data[1])\n",
    "        \n",
    "        torch_pred = torch_model(data)\n",
    "        torch_pred = torch_pred.detach().cpu().numpy()\n",
    "#         print(f\"torch_pred shape: {torch_pred.shape}\")\n",
    "        hls_pred = hls_model.predict(data.hls_data)\n",
    "#         print(f\"hls_pred shape: {hls_pred.shape}\")\n",
    "        MSE = mean_squared_error(torch_pred, hls_pred)\n",
    "#         print(f\"MSE: {MSE}\")\n",
    "    #     get_meanNstd(data,torch_pred, hls_pred)\n",
    "    #     print(f\"MSE: {MSE}\")\n",
    "        MSE_l.append(MSE)\n",
    "    #     MAPE = mean_absolute_percentage_error(torch_pred, hls_pred)\n",
    "    #     print(f\"MAPE: {MAPE}\")\n",
    "#         break\n",
    "MSE_l = np.array(MSE_l)\n",
    "print(f\"MSE_l :{MSE_l}\")\n",
    "print(f\"MSE means: {np.mean(MSE_l)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fe5a646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(578745,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5741bccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch_model.state_dict()\n",
    "with open('MSE_l.npy', 'wb') as f:\n",
    "    np.save(f, MSE_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3abe02c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53405494",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7b34b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
