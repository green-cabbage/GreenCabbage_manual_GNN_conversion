{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1bc2776",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9b384d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: Conv1d\n",
      "func: <function parse_conv1d_layer at 0x7f1c8ba67f80>\n",
      "layer: Conv2d\n",
      "func: <function parse_conv2d_layer at 0x7f1c8ba70050>\n",
      "layer: Linear\n",
      "func: <function parse_linear_layer at 0x7f1c8ba70200>\n",
      "layer: Softmax\n",
      "func: <function parse_activation_layer at 0x7f1c8ba70170>\n",
      "layer: ReLU\n",
      "func: <function parse_activation_layer at 0x7f1c8ba70170>\n",
      "layer: BatchNorm2d\n",
      "func: <function parse_batchnorm_layer at 0x7f1c8ba700e0>\n",
      "layer: BatchNorm1d\n",
      "func: <function parse_batchnorm_layer at 0x7f1c8ba700e0>\n",
      "layer: NodeBlock\n",
      "func: <function parse_NodeBlock at 0x7f1c8ba7d290>\n",
      "layer: EdgeBlock\n",
      "func: <function parse_EdgeBlock at 0x7f1c8ba7d320>\n",
      "layer: EdgeAggregate\n",
      "func: <function parse_EdgeAggregate at 0x7f1c8ba7d3b0>\n",
      "layer: ResidualBlock\n",
      "func: <function parse_ResidualBlock at 0x7f1c8ba7d440>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from hls4ml.utils.config import config_from_pyg_model\n",
    "from hls4ml.converters import convert_from_pyg_model\n",
    "# module_path = os.path.abspath(os.path.join('../pyg_to_hls_hls4ml/hls4ml/utils'))\n",
    "# print(module_path)\n",
    "# if module_path not in sys.path:\n",
    "#     sys.path.append(module_path)\n",
    "# from config import config_from_pyg_model\n",
    "\n",
    "# module_path = os.path.abspath(os.path.join('../pyg_to_hls_hls4ml/hls4ml'))\n",
    "# print(module_path)\n",
    "# if module_path not in sys.path:\n",
    "#     sys.path.append(module_path)\n",
    "# from converters import convert_from_pyg_model\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# locals\n",
    "from utils.models.interaction_network_pyg import InteractionNetwork\n",
    "from model_wrappers import model_wrapper\n",
    "from utils.data.dataset_pyg import GraphDataset\n",
    "from utils.data.fix_graph_size import fix_graph_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb118bac",
   "metadata": {},
   "source": [
    "### PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24082fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R1: RelationalModel(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=40, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=40, out_features=40, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=40, out_features=4, bias=True)\n",
      "  )\n",
      ")\n",
      "R1.layers: Sequential(\n",
      "  (0): Linear(in_features=10, out_features=40, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=40, out_features=40, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=40, out_features=4, bias=True)\n",
      ")\n",
      "R1.layers.0: Linear(in_features=10, out_features=40, bias=True)\n",
      "R1.layers.1: ReLU()\n",
      "R1.layers.2: Linear(in_features=40, out_features=40, bias=True)\n",
      "R1.layers.3: ReLU()\n",
      "R1.layers.4: Linear(in_features=40, out_features=4, bias=True)\n",
      "O: ObjectModel(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=40, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=40, out_features=40, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=40, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "O.layers: Sequential(\n",
      "  (0): Linear(in_features=7, out_features=40, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=40, out_features=40, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=40, out_features=3, bias=True)\n",
      ")\n",
      "O.layers.0: Linear(in_features=7, out_features=40, bias=True)\n",
      "O.layers.1: ReLU()\n",
      "O.layers.2: Linear(in_features=40, out_features=40, bias=True)\n",
      "O.layers.3: ReLU()\n",
      "O.layers.4: Linear(in_features=40, out_features=3, bias=True)\n",
      "R2: RelationalModel(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=40, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=40, out_features=40, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=40, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "R2.layers: Sequential(\n",
      "  (0): Linear(in_features=10, out_features=40, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=40, out_features=40, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=40, out_features=1, bias=True)\n",
      ")\n",
      "R2.layers.0: Linear(in_features=10, out_features=40, bias=True)\n",
      "R2.layers.1: ReLU()\n",
      "R2.layers.2: Linear(in_features=40, out_features=40, bias=True)\n",
      "R2.layers.3: ReLU()\n",
      "R2.layers.4: Linear(in_features=40, out_features=1, bias=True)\n",
      "res_block: ResidualBlock()\n"
     ]
    }
   ],
   "source": [
    "torch_model = InteractionNetwork(aggr=\"add\", flow=\"source_to_target\", hidden_size=40)\n",
    "# torch_model_dict = torch.load(\"trained_models//IN_pyg_small_add_source_to_target_40_state_dict.pt\")\n",
    "# torch_model.load_state_dict(torch_model_dict)\n",
    "\n",
    "for name, submodule in torch_model.named_modules():\n",
    "    if name != \"\":\n",
    "        print(f\"{name}: {submodule}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2293596d",
   "metadata": {},
   "source": [
    "We can see that this specific GNN is composed of 3 submodules:\n",
    "- The first submodule, \"R1\", is a \"RelationalModel\" a.k.a. an \"EdgeBlock\"\n",
    "- The second submodule, \"O\", is an \"ObjectModel\" a.k.a. a \"NodeBlock\"\n",
    "- The third submodule, \"R2\" is another \"RelationalModel\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e49e754",
   "metadata": {},
   "source": [
    "### HLS Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e798bed7",
   "metadata": {},
   "source": [
    "hls4ml cannot infer the *order* in which these submodules are called within the pytorch model's \"forward()\" function. We have to manually define this information in the form of an ordered-dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18b64da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward_dict: defines the order in which graph-blocks are called in the model's 'forward()' method\n",
    "forward_dict = OrderedDict()\n",
    "forward_dict[\"R1\"] = \"EdgeBlock\"\n",
    "forward_dict[\"O\"] = \"NodeBlock\"\n",
    "forward_dict[\"res_block\"] = \"ResidualBlock\"\n",
    "forward_dict[\"R2\"] = \"EdgeBlock\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "6dca8d9f",
   "metadata": {},
   "source": [
    "hls4ml creates a hardware implementation of the GNN, which can only be represented using fixed-size arrays. This restriction also applies to the inputs and outputs of the GNN, so we must define the size of the graphs that this hardware GNN can take as input**, again in the form of a dictionary. \n",
    "\n",
    "**Graphs of a different size can be padded or truncated to the appropriate size using the \"fix_graph_size\" function. In this notebook, padding/truncation is  done in the \"Data\" cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baa5a9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dims = {\n",
    "        \"n_node\": 28,\n",
    "        \"n_edge\": 37,\n",
    "        \"node_dim\": 3,\n",
    "        \"edge_dim\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "623f2192",
   "metadata": {},
   "source": [
    "Armed with our pytorch model and these two dictionaries**, we can create the HLS model. \n",
    "\n",
    "**If there is some activation function after the output of the final GNN-submodule, we also have to pass the type of this activation through the \"activate_final\" parameter of \"convert_from_pyg_model\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da3c3fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank: 2\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"test_GNN\"\n",
    "config = config_from_pyg_model(torch_model,\n",
    "                                   default_precision=\"ap_fixed<32,12>\",\n",
    "                                   default_index_precision='ap_uint<16>', \n",
    "                                   default_reuse_factor=1)\n",
    "hls_model = convert_from_pyg_model(torch_model,\n",
    "                                       n_edge=graph_dims['n_edge'],\n",
    "                                       n_node=graph_dims['n_node'],\n",
    "                                       edge_dim=graph_dims['edge_dim'],\n",
    "                                       node_dim=graph_dims['node_dim'],\n",
    "                                       forward_dictionary=forward_dict, \n",
    "                                       activate_final='sigmoid',\n",
    "                                       output_dir=output_dir,\n",
    "                                       hls_config=config)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c4b1ec2",
   "metadata": {},
   "source": [
    "The user can also define different fixed-point precision, integer/index precision, or reuse-factor parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59aa1957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: {'Model': {'Precision': 'ap_fixed<32,12>', 'IndexPrecision': 'ap_uint<16>', 'ReuseFactor': 8, 'Strategy': 'Latency'}}\n",
      "rank: 2\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"test_GNN\"\n",
    "config = config_from_pyg_model(torch_model,\n",
    "                                   default_precision=\"ap_fixed<32,12>\",\n",
    "                                   default_index_precision='ap_uint<16>', \n",
    "                                   default_reuse_factor=8)\n",
    "print(f\"config: {config}\")\n",
    "hls_model = convert_from_pyg_model(torch_model,\n",
    "                                       n_edge=graph_dims['n_edge'],\n",
    "                                       n_node=graph_dims['n_node'],\n",
    "                                       edge_dim=graph_dims['edge_dim'],\n",
    "                                       node_dim=graph_dims['node_dim'],\n",
    "                                       forward_dictionary=forward_dict, \n",
    "                                       activate_final='sigmoid',\n",
    "                                       output_dir=output_dir,\n",
    "                                       hls_config=config)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5859accc",
   "metadata": {},
   "source": [
    "hls_model.compile() builds the C-function for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7da705cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing HLS project\n",
      "layer name: node_attr. func: None\n",
      "layer name: edge_attr. func: None\n",
      "layer name: edge_index. func: None\n",
      "layer name: R1. func: ['nnet::edgeblock<input2_t, input3_t, layer4_t, config4>(node_attr, edge_attr, edge_index, layer4_out, R1_w0, R1_b0, R1_w1, R1_b1, R1_w2, R1_b2, R1_w3, R1_b3);']\n",
      "layer name: aggr5. func: ['nnet::edge_aggregate<input2_t, input3_t, layer5_t, aggregation_config5>(layer4_out, edge_index, layer5_out);']\n",
      "nodeblock index: 6\n",
      "nodeblock _function_template: nnet::nodeblock<{input_t}, {output_t}, {config}>({node_attr}, {edge_attr_aggr}, {out}, {w0}, {b0}, {w1}, {b1}, {w2}, {b2}, {w3}, {b3});\n",
      "nodeblock final _function_template: nnet::nodeblock<input_t, layer6_t, config6>(node_attr, layer5_out, layer6_out, O_w0, O_b0, O_w1, O_b1, O_w2, O_b2, O_w3, O_b3);\n",
      "layer name: O. func: ['nnet::nodeblock<input_t, layer6_t, config6>(node_attr, layer5_out, layer6_out, O_w0, O_b0, O_w1, O_b1, O_w2, O_b2, O_w3, O_b3);']\n",
      "residualBlock template: nnet::{merge}<{input1_t}, {input2_t}, {output_t}, {config}>({input1}, {input2}, {output});\n",
      "final residualBlock template: nnet::residualBlock<layer6_t, layer6_t, layer7_t, config7>(layer6_out, layer6_out, layer7_out);\n",
      "layer name: aggr7. func: ['nnet::residualBlock<layer6_t, layer6_t, layer7_t, config7>(layer6_out, layer6_out, layer7_out);']\n",
      "layer name: R2. func: ['nnet::edgeblock<input2_t, input3_t, layer8_t, config8>(layer7_out, layer4_out, edge_index, layer8_out, R2_w0, R2_b0, R2_w1, R2_b1, R2_w2, R2_b2, R2_w3, R2_b3);']\n",
      "layer name: final_act. func: ['nnet::sigmoid<layer8_t, result_t, sigmoid_config9>(layer8_out, layer9_out);']\n",
      "params['n_elem']: N_NODE*LAYER6_OUT_DIM\n",
      "nodeblock index: 6\n",
      "nodeblock _function_template: nnet::nodeblock<{input_t}, {output_t}, {config}>({node_attr}, {edge_attr_aggr}, {out}, {w0}, {b0}, {w1}, {b1}, {w2}, {b2}, {w3}, {b3});\n",
      "nodeblock final _function_template: nnet::nodeblock<input_t, layer6_t, config6>(node_attr, layer5_out, layer6_out, O_w0, O_b0, O_w1, O_b1, O_w2, O_b2, O_w3, O_b3);\n",
      "residualBlock template: nnet::{merge}<{input1_t}, {input2_t}, {output_t}, {config}>({input1}, {input2}, {output});\n",
      "final residualBlock template: nnet::residualBlock<layer6_t, layer6_t, layer7_t, config7>(layer6_out, layer6_out, layer7_out);\n",
      "Done\n",
      "lib_name: firmware/myproject-EAb01ecc.so\n"
     ]
    }
   ],
   "source": [
    "hls_model.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e678b04",
   "metadata": {},
   "source": [
    "# Evaluation and prediction: hls_model.predict(input)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e31a4c5a",
   "metadata": {},
   "source": [
    "If your model takes a non-singular input (e.g. node attributes, edge attributes, and an edge index), then you should pass it as a list (e.g. [node_attr, edge_attr, edge_index]). See the \"data_wrapper\" class, and note that the hls_model.predict() method is used on the data.hls_data attribute. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661c3eaa",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df4856a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDS: [0 1 2 3 4 5 6 7 8 9]\n",
      "graphs length: 10\n",
      "writing test bench data for 1st graph\n"
     ]
    }
   ],
   "source": [
    "class data_wrapper(object):\n",
    "    def __init__(self, node_attr, edge_attr, edge_index, target):\n",
    "        self.x = node_attr\n",
    "        self.edge_attr = edge_attr\n",
    "        self.edge_index = edge_index.transpose(0,1)\n",
    "\n",
    "        node_attr, edge_attr, edge_index = self.x.detach().cpu().numpy(), self.edge_attr.detach().cpu().numpy(), self.edge_index.transpose(0, 1).detach().cpu().numpy().astype(np.float32)\n",
    "        node_attr, edge_attr, edge_index = np.ascontiguousarray(node_attr), np.ascontiguousarray(edge_attr), np.ascontiguousarray(edge_index)\n",
    "        self.hls_data = [node_attr, edge_attr, edge_index]\n",
    "\n",
    "        self.target = target\n",
    "        self.np_target = np.reshape(target.detach().cpu().numpy(), newshape=(target.shape[0],))\n",
    "\n",
    "def load_graphs(graph_indir, graph_dims, n_graphs):\n",
    "    graph_files = np.array(os.listdir(graph_indir))\n",
    "    graph_files = np.array([os.path.join(graph_indir, graph_file)\n",
    "                            for graph_file in graph_files])\n",
    "    n_graphs_total = len(graph_files)\n",
    "    IDs = np.arange(n_graphs_total)\n",
    "    print(f\"IDS: {IDs}\")\n",
    "    dataset = GraphDataset(graph_files=graph_files[IDs])\n",
    "\n",
    "    graphs = []\n",
    "    for data in dataset[:n_graphs]:\n",
    "        node_attr, edge_attr, edge_index, target, bad_graph = fix_graph_size(data.x, data.edge_attr, data.edge_index,\n",
    "                                                                             data.y,\n",
    "                                                                             n_node_max=graph_dims['n_node'],\n",
    "                                                                             n_edge_max=graph_dims['n_edge'])\n",
    "#         if not bad_graph:\n",
    "#             graphs.append(data_wrapper(node_attr, edge_attr, edge_index, target))\n",
    "        graphs.append(data_wrapper(node_attr, edge_attr, edge_index, target))\n",
    "    print(f\"graphs length: {len(graphs)}\")\n",
    "\n",
    "    print(\"writing test bench data for 1st graph\")\n",
    "    data = graphs[0]\n",
    "    node_attr, edge_attr, edge_index = data.x.detach().cpu().numpy(), data.edge_attr.detach().cpu().numpy(), data.edge_index.transpose(\n",
    "        0, 1).detach().cpu().numpy().astype(np.int32)\n",
    "    os.makedirs('tb_data', exist_ok=True)\n",
    "    input_data = np.concatenate([node_attr.reshape(1, -1), edge_attr.reshape(1, -1), edge_index.reshape(1, -1)], axis=1)\n",
    "    np.savetxt('tb_data/input_data.dat', input_data, fmt='%f', delimiter=' ')\n",
    "\n",
    "    return graphs\n",
    "\n",
    "\n",
    "graph_indir = \"trackml_data/processed_plus_pyg_small\"\n",
    "graph_dims = {\n",
    "        \"n_node\": 28,\n",
    "        \"n_edge\": 37,\n",
    "        \"node_dim\": 3,\n",
    "        \"edge_dim\": 4\n",
    "    }\n",
    "graphs = load_graphs(graph_indir, graph_dims, n_graphs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "374f8f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graphs len: 10\n",
      "<class 'list'>\n",
      "data.x shape:torch.Size([28, 3])\n",
      "torch pred shape: (37,)\n",
      "hls_pred.shape: (37,)\n",
      "MSE: 4.282571808289504e-06\n",
      "[3.6702752e-03 3.6845803e-03 3.6625862e-03 3.6818981e-03 4.3183565e-05\n",
      " 5.9455633e-05 4.4107437e-05 6.4074993e-05 3.8875043e-03 8.3446503e-07\n",
      " 3.8879812e-03 2.5629997e-06 3.7461519e-05 3.5077333e-05 3.6597252e-05\n",
      " 3.8325787e-05 3.8305521e-03 9.9629164e-05 9.7513199e-05 9.5009804e-05\n",
      " 9.7095966e-05 3.8990676e-03 8.5830688e-05 8.6992979e-05 8.0704689e-05\n",
      " 8.4966421e-05 3.8801134e-03 6.5803528e-05 6.9856644e-05 6.0498714e-05\n",
      " 6.7293644e-05 3.8344860e-03 8.3237886e-05 8.5562468e-05 7.5519085e-05\n",
      " 8.0585480e-05 3.8008392e-03]\n",
      "[0.49242347 0.49240917 0.49243116 0.49241185 0.49214432 0.49212804\n",
      " 0.4921434  0.49212343 0.49220625 0.49218833 0.49220577 0.49218494\n",
      " 0.49215004 0.49215242 0.4921509  0.49214917 0.4922632  0.49208787\n",
      " 0.49209    0.4920925  0.4920904  0.49219468 0.49210167 0.4921005\n",
      " 0.4921068  0.49210253 0.49221364 0.4921217  0.49211764 0.492127\n",
      " 0.4921202  0.49225926 0.49210426 0.49210194 0.49211198 0.4921069\n",
      " 0.4922929 ]\n",
      "[0.49609375 0.49609375 0.49609375 0.49609375 0.4921875  0.4921875\n",
      " 0.4921875  0.4921875  0.49609375 0.4921875  0.49609375 0.4921875\n",
      " 0.4921875  0.4921875  0.4921875  0.4921875  0.49609375 0.4921875\n",
      " 0.4921875  0.4921875  0.4921875  0.49609375 0.4921875  0.4921875\n",
      " 0.4921875  0.4921875  0.49609375 0.4921875  0.4921875  0.4921875\n",
      " 0.4921875  0.49609375 0.4921875  0.4921875  0.4921875  0.4921875\n",
      " 0.49609375]\n"
     ]
    }
   ],
   "source": [
    "data = graphs[0]\n",
    "print(f\"graphs len: {len(graphs)}\")\n",
    "print(type(data.hls_data))\n",
    "print(f\"data.x shape:{data.x.shape}\")\n",
    "torch_pred = torch_model(data)\n",
    "torch_pred = torch_pred.detach().cpu().numpy().flatten()\n",
    "print(f\"torch pred shape: {torch_pred.shape}\")\n",
    "hls_pred = hls_model.predict(data.hls_data)\n",
    "print(f\"hls_pred.shape: {hls_pred.shape}\")\n",
    "MSE = mean_squared_error(torch_pred, hls_pred)\n",
    "print(f\"MSE: {MSE}\")\n",
    "\n",
    "print((np.abs(torch_pred- hls_pred)))\n",
    "print(torch_pred)\n",
    "print(hls_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63133890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meanNstd(data, torch_pred, hls_pred):\n",
    "#     print(data.x)\n",
    "    node_mean = torch.mean(data.x)\n",
    "    print(f\"node_mean: {node_mean}\")\n",
    "    node_std = torch.std(data.x)\n",
    "    print(f\"node_std: {node_std}\")\n",
    "    node_abs_max = torch.max(torch.abs(data.x))\n",
    "    print(f\"node_abs_max: {node_abs_max}\")\n",
    "    edge_mean = torch.mean(data.edge_attr)\n",
    "    print(f\"edge_mean: {edge_mean}\")\n",
    "    edge_std = torch.std(data.edge_attr)\n",
    "    print(f\"edge_std: {edge_std}\")\n",
    "    edge_abs_max = torch.max(torch.abs(data.edge_attr))\n",
    "    print(f\"edge_abs_max: {edge_abs_max}\")\n",
    "    torch_pred_mean = np.mean(torch_pred)\n",
    "    print(f\"torch_pred_mean: {torch_pred_mean}\")\n",
    "    torch_pred_std = np.std(torch_pred)\n",
    "    print(f\"torch_pred_std: {torch_pred_std}\")\n",
    "    torch_pred_abs_max = np.max(np.abs(torch_pred))\n",
    "    print(f\"torch_pred_abs_max: {torch_pred_abs_max}\")\n",
    "    hls_pred_mean = np.mean(hls_pred)\n",
    "    print(f\"hls_pred_mean: {hls_pred_mean}\")\n",
    "    hls_pred_std = np.std(hls_pred)\n",
    "    print(f\"hls_pred_std: {hls_pred_std}\")\n",
    "    hls_pred_abs_max = np.max(np.abs(hls_pred))\n",
    "    print(f\"hls_pred_abs_max: {hls_pred_abs_max}\")\n",
    "    diff_mean = np.mean(torch_pred- hls_pred)\n",
    "    print(f\"diff_mean: {diff_mean}\")\n",
    "    diff_std = np.std(torch_pred- hls_pred)\n",
    "    print(f\"diff_std: {diff_std}\")\n",
    "    diff_abs_max = np.max(np.abs(torch_pred- hls_pred))\n",
    "    print(f\"diff_abs_max: {diff_abs_max}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a300c466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall MSE: 8.005933523236308e-06\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "MSE_l = []\n",
    "for data in graphs:\n",
    "#     print(f\"graphs len: {len(graphs)}\")\n",
    "#     print(type(data.hls_data))\n",
    "#     print(f\"data.x shape:{data.x.shape}\")\n",
    "    torch_pred = torch_model(data)\n",
    "    torch_pred = torch_pred.detach().cpu().numpy().flatten()\n",
    "#     print(f\"torch pred shape: {torch_pred.shape}\")\n",
    "    hls_pred = hls_model.predict(data.hls_data)\n",
    "#     print(f\"hls_pred.shape: {hls_pred.shape}\")\n",
    "    MSE = mean_squared_error(torch_pred, hls_pred)\n",
    "#     get_meanNstd(data, torch_pred, hls_pred)\n",
    "#     print(f\"MSE: {MSE}\")\n",
    "    MSE_l.append(MSE)\n",
    "#     MAPE = mean_absolute_percentage_error(torch_pred, hls_pred)\n",
    "#     print(f\"MAPE: {MAPE}\")\n",
    "\n",
    "MSE_l = np.array(MSE_l)\n",
    "print(f\"overall MSE: {np.mean(MSE_l)}\")\n",
    "print(MSE_l.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b4fd437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 3)\n",
      "(37, 4)\n",
      "(37, 2)\n",
      "torch.Size([2, 37])\n"
     ]
    }
   ],
   "source": [
    "for data_instance in data.hls_data:\n",
    "    print(data_instance.shape)\n",
    "    \n",
    "print(data.edge_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e49ea99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class data_wrapper_tau3mu:\n",
    "#     def __init__(self, node_attr, edge_attr, edge_index, target):\n",
    "#         self.x = node_attr\n",
    "#         self.edge_attr = edge_attr\n",
    "#         self.edge_index = edge_index.transpose(0,1)\n",
    "\n",
    "#         node_attr, edge_attr, edge_index = self.x.detach().cpu().numpy(), self.edge_attr.detach().cpu().numpy(), self.edge_index.transpose(0, 1).detach().cpu().numpy().astype(np.float32)\n",
    "#         node_attr, edge_attr, edge_index = np.ascontiguousarray(node_attr), np.ascontiguousarray(edge_attr), np.ascontiguousarray(edge_index)\n",
    "#         self.hls_data = [node_attr, edge_attr, edge_index]\n",
    "\n",
    "#         self.target = target\n",
    "#         self.np_target = np.reshape(target.detach().cpu().numpy(), newshape=(target.shape[0],))\n",
    "\n",
    "def load_graphs_tau3mu(data_loader, graph_dims:dict):\n",
    "    \"\"\"\n",
    "    params:\n",
    "    dataloader: pyg dataloader with custom Tau3MuDataset as its dataset\n",
    "    graph_dims: \n",
    "        graph_dims.keys = [\"n_node\": max number of nodes allowed in a graph/batch,\n",
    "            \"n_edge\": max number of edges allowed in a graph/batch,\n",
    "            \"node_dim\": feature dim of node,\n",
    "            \"edge_dim\": feature dim of edge\n",
    "        ]\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    graphs = []\n",
    "    \n",
    "    i = 0\n",
    "    for data in data_loader:\n",
    "        data.edge_index = data.edge_index.t()#transpose the edge_index\n",
    "        n_edges = data.edge_attr.shape[0]\n",
    "#         print((data.y.shape))\n",
    "#         print(n_edges)\n",
    "        data.y = data.y.expand(1, n_edges).flatten()\n",
    "#         print(f\"data.y.shape: {data.y.shape}\")\n",
    "        node_attr, edge_attr, edge_index, target, bad_graph = fix_graph_size(data.x, \n",
    "                                                                             data.edge_attr, \n",
    "                                                                             data.edge_index,\n",
    "                                                                             data.y,\n",
    "                                                                             n_node_max=graph_dims['n_node'],\n",
    "                                                                             n_edge_max=graph_dims['n_edge'])\n",
    "        target = torch.flatten(target)# flatten target to 1d array\n",
    "        if not bad_graph:\n",
    "            graphs.append(data_wrapper(node_attr, edge_attr, edge_index, target))\n",
    "        i +=1\n",
    "        \n",
    "    print(f\"n_graphs: {len(graphs)}\")\n",
    "\n",
    "    print(\"writing test bench data for 1st graph\")\n",
    "    data = graphs[0]\n",
    "    node_attr, edge_attr, edge_index = data.x.detach().cpu().numpy(), data.edge_attr.detach().cpu().numpy(), data.edge_index.transpose(\n",
    "        0, 1).detach().cpu().numpy().astype(np.int32)\n",
    "    os.makedirs('tb_data', exist_ok=True)\n",
    "    input_data = np.concatenate([node_attr.reshape(1, -1), edge_attr.reshape(1, -1), edge_index.reshape(1, -1)], axis=1)\n",
    "    np.savetxt('tb_data/input_data.dat', input_data, fmt='%f', delimiter=' ')\n",
    "\n",
    "    return graphs\n",
    "\n",
    "\n",
    "graph_indir = \"trackml_data/processed_plus_pyg_small\"\n",
    "# graph_indir = \"/home/swissman777/projects/Tau3MuGNNs/data\"\n",
    "graph_dims = {\n",
    "        \"n_node\": 28,\n",
    "        \"n_edge\": 37,\n",
    "        \"node_dim\": 3,\n",
    "        \"edge_dim\": 4\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9b585b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': <torch_geometric.loader.dataloader.DataLoader object at 0x7f1c5880e710>, 'valid': <torch_geometric.loader.dataloader.DataLoader object at 0x7f1c58804fd0>, 'test': <torch_geometric.loader.dataloader.DataLoader object at 0x7f1c58804790>}\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "from torch_geometric.loader.dataloader import DataLoader\n",
    "from utils.dataset import Tau3MuDataset\n",
    "\n",
    "# with open('./trackml_data/data_loaders.pickle', 'rb') as file:\n",
    "with open('./trackml_data/data_loaders_batch_size_1.pickle', 'rb') as file:\n",
    "    data_loaders= pkl.load(file) #, x_dim, edge_attr_dim \n",
    "\n",
    "print(data_loaders)\n",
    "# for stage in data_loaders.keys():\n",
    "#     for data in data_loaders[stage]:\n",
    "#         print(data.x.shape)\n",
    "#         print(data.edge_index.shape)\n",
    "#         print(data.edge_attr.shape)\n",
    "\n",
    "# print(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62330a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_graphs: 332406\n",
      "writing test bench data for 1st graph\n",
      "n_graphs: 71226\n",
      "writing test bench data for 1st graph\n",
      "n_graphs: 175113\n",
      "writing test bench data for 1st graph\n",
      "MSE means: 3.849876520689577e-06\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "MSE_l = []\n",
    "stages = [\"train\", \"valid\", \"test\"]\n",
    "for stage in stages:\n",
    "    graphs = load_graphs_tau3mu(data_loaders[stage], graph_dims)\n",
    "    for data in graphs:\n",
    "        torch_pred = torch_model(data)\n",
    "        torch_pred = torch_pred.detach().cpu().numpy()\n",
    "        hls_pred = hls_model.predict(data.hls_data)\n",
    "        MSE = mean_squared_error(torch_pred, hls_pred)\n",
    "    #     get_meanNstd(data,torch_pred, hls_pred)\n",
    "    #     print(f\"MSE: {MSE}\")\n",
    "        MSE_l.append(MSE)\n",
    "    #     MAPE = mean_absolute_percentage_error(torch_pred, hls_pred)\n",
    "    #     print(f\"MAPE: {MAPE}\")\n",
    "MSE_l = np.array(MSE_l)\n",
    "print(f\"MSE means: {np.mean(MSE_l)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fe5a646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(578745,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5741bccd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
